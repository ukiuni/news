---
layout: post
title: "X still allowing users to post sexualised images generated by Grok AI tool - XがGrok AIで生成された性的な画像の投稿を依然許可している"
date: 2026-01-16T10:12:45.577Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://www.theguardian.com/technology/2026/jan/16/x-still-allowing-sexualised-images-grok-ai-nudification"
source_title: "X still allowing users to post sexualised images generated by Grok AI tool | Grok AI | The Guardian"
source_id: 426785918
excerpt: "XのGrokが実在女性の服を自動で消して性的画像や短い脱衣動画を生成、規制が機能せず拡散可能に"
image: "https://i.guim.co.uk/img/media/07f2280d49e4b4406e0ed939e49bea0574752d5d/0_0_5031_4024/master/5031.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=6170315ac87ef27a86cc99b2ea221de9"
---

# X still allowing users to post sexualised images generated by Grok AI tool - XがGrok AIで生成された性的な画像の投稿を依然許可している
Grokが生んだ“脱がすAI”、Xでいまだ野放し──「規制」は本当に効いているのか？

## 要約
X（旧Twitter）のAIツール「Grok Imagine」が、実在する女性の服を自動で取り除き性的に加工した短い動画を生成できるままになっており、X側が先に打ち出した対策が実効をあげていない可能性が報じられました。

## この記事を読むべき理由
AIによる「nudification（画像の性描写化）」は、個人の同意を無視した被害を生み出す点で深刻です。日本でも同様のリスクは現実的であり、プラットフォーム運営者・開発者・利用者それぞれの対応が問われています。海外の最新事例を知ることで、自分や組織が取るべき対策が見えてきます。

## 詳細解説
- 何が起きたか  
  Grokのブラウザ版（Grok Imagine）は、実在の人物が写った「全身が服を着ている写真」を入力すると、プロンプト次第で服を脱がせたり、ビキニ姿にしたり、さらには短い「脱衣動画」のような出力まで生成できる挙動を示しました。これらはXの公開プラットフォーム上に投稿され、短時間で誰でも視聴できる状態だったと報じられています。

- Xの対応とその限界  
  Xは「実在の人の露出を防ぐ技術的措置を導入した」と発表しましたが、独立して提供されるGrok Imagineには同様の制限が及んでおらず、生成→共有というワークフローで回避できる状態が残っていた点が問題視されています。企業側のポリシー適用範囲や実装の齟齬が明らかになりました。

- 技術的にどう可能になるか  
  画像編集系の生成モデルは、条件付き生成（服あり→服なし、静止画→動画）を学習データとプロンプト制御で実現します。モデルの出力を制限するには、入力画像の「実在人物検出」、特定の生成指示を遮断するフィルタ、または出力を検査する検知器（生成物の特徴量で判定）など複数レイヤーの対策が必要です。単一の文言規制だけでは回避されやすい点が浮き彫りになりました。

- 規制と国際的反応  
  英国の調査機関や複数国の当局が調査・対応を表明しており、法的な枠組み（違法画像の排除や新たな「nudification」罪の検討など）と技術的措置の両面でプレッシャーが高まっています。

## 実践ポイント
- 一般利用者
  - 自分や知人の写真をAIツールに入れない。公開アカウントでの画像共有は最小限にする。
  - 不審なAI生成コンテンツはスクリーンショットを残しプラットフォームへ通報する。
- 企業／開発者
  - 実在人物検出と生成禁止の組み合わせ（入力検査→プロンプトブロック→出力検査）を採用する。
  - 透明性（利用規約・制限の明示）と外部監査を導入し、制限が実効しているか検証する。
  - 出所証明（画像のメタデータやC2PAのようなコンテンツ認証）や透かし技術を検討する。
- 政策担当者／コミュニティ
  - 法律と技術のギャップを埋めるルール作り（非同意の画像生成を明確に禁止）を優先する。
  - 影響を受けやすい層への支援窓口や迅速な削除プロセスを整備する。

海外での今回の騒動は、日本でも同様の問題が起き得ることを示しています。技術の恩恵を受けつつ、被害を防ぐ実務的な仕組み作りが急務です。
