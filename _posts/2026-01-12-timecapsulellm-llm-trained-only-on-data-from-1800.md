---
layout: post
title: "TimeCapsuleLLM: LLM trained only on data from 1800-1875 - 1800〜1875年の資料だけで学習した言語モデル「TimeCapsuleLLM」"
date: 2026-01-12T17:20:34.575Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://github.com/haykgrigo3/TimeCapsuleLLM"
source_title: "GitHub - haykgrigo3/TimeCapsuleLLM: A LLM trained only on data from certain time periods to reduce modern bias"
source_id: 46590280
excerpt: "1800–1875年ロンドン資料だけで学習したLLMが当時の語彙と文体を再現し、研究や観光に新価値を提案"
image: "https://opengraph.githubassets.com/a952da8dddc9fb3210f8b54051a2f3bb111fdc1d37b9863ddbb27d893a313c4b/haykgrigo3/TimeCapsuleLLM"
---

# TimeCapsuleLLM: LLM trained only on data from 1800-1875 - 1800〜1875年の資料だけで学習した言語モデル「TimeCapsuleLLM」
魅力的タイトル: 「AIが“その時代そのもの”になる — 1800〜1875年ロンドンだけで学習した言語モデルの衝撃」

## 要約
TimeCapsuleLLMは、特定の時代・地域（例：1800–1875年のロンドン）の公開資料だけで一から学習させ、現代バイアスを排して当時の語彙・文体・世界観を再現しようとする実験的LLMプロジェクトです。複数バージョンを経て語彙や文法性は改善されるものの、OCRノイズや事実誤認（ハルシネーション）は残ります。

## この記事を読むべき理由
- 歴史言語表現や時代的視点を「本物に近い形」で扱えるAIは、研究・教育・博物館アプリで新しい価値を生みます。  
- 日本でも明治期や大正期などを対象に同様の手法を取れば、史料解析や文化継承、観光向けチャットボットの差別化が可能です。  
- 実際にどうデータを集め、トークナイザーを作り、学習を進めるかの実践的な手順が公開されています（nanoGPTやPhiベースの実装参考）。

## 詳細解説
- コンセプト（Selective Temporal Training）  
  すべての学習データを「ある時代・場所の資料だけ」に限定することで、現代語やネット由来の常識に引きずられないモデルを作る手法。Fine-tuneではなくスクラッチ学習を選ぶ理由は、事前学習済みモデルに残る現代的知識を排除するためです。

- データとバージョン  
  v0: 約187MB（16Mパラメータ） — 初期は文が不明瞭。  
  v0.5: ~435MB（123M） — ヴィクトリア朝風の文体に改善。  
  v1: ~6.25GB（700M） — 実在の出来事を結びつけられる例が出始める。v1はPhi 1.5ベース。  
  v2mini-eval: 15GBサンプル（300M） — トークナイザー問題やOCRノイズで文字間隔が乱れる出力が観測。最終的に90GBコーパス（1800–1875年ロンドン、136k文書）を目標にしています。

- 技術スタックと注意点  
  コアはnanoGPT（Andrej Karpathyのリポジトリ）やPhi 1.5を利用。重要な工程は「史料収集→クリーニング→カスタムトークナイザー作成→学習」。OCRノイズ、近代注釈、ヘッダ/フッタの除去が品質に直結します。トークナイザーの不備は出力の分断（単語がバラバラになる）を招くため、十分な検証が必要です。

- 性能と限界  
  文体は時代を反映するが、事実関係の誤認（ハルシネーション）やOCR由来のノイズは残る。訓練データが少ないと文法や一貫性が崩れやすい。モデルはあくまで「その時代の語り口」を模するもので、正確な歴史的事実の保証はない点を理解する必要があります。

## 実践ポイント
- まずは小さなサンプルで試す：15GB程度のクリーンな史料でトークナイザーと小モデルを検証する。  
- データ準備の重要性：Project GutenbergやInternet Archiveのテキストはヘッダ/フッタや注釈、OCRミスを自分で取り除く。日本語なら青空文庫などの文字コード・改行を整え、形態素解析器（MeCab / Sudachi / SentencePiece）を検討する。  
- トークナイザーを自作する：英語ではBPEやSentencePiece、モデル付属スクリプト（train_tokenizer.py）を使う。日本語はサブワード粒度と語彙サイズの設計が肝。  
- 計算資源の見積もり：小モデルはローカルGPUで可能だが、大規模学習はクラウドのA100等を利用。学習ステップ数・データ量と算出リソースを事前に計画する。  
- 評価とバイアス確認：生成文の時代適合性だけでなく誤情報・偏りの評価レポートを作る。用途によっては人間による検閲やファクトチェックを導入する。  
- 日本向け応用案：明治期コーパスで「当時の言葉遣いで答える観光案内」「教育用の歴史対話エージェント」「デジタル史料の文体復元」などが考えられる。

最後に：TimeCapsuleLLMは「AIを時代へ閉じ込める」挑戦の良いケーススタディです。日本語史料で同様の手法を試せば、新しいデジタル人文学や地域文化の発信につながる可能性があります。興味があるなら、まずは小さなコーパスでトークナイザーの試作から始めてください。
