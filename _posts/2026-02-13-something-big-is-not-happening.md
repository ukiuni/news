---
layout: post
title: "Something Big Is (Not) Happening - 何か大きなことが（起きているわけではない）"
date: 2026-02-13T22:22:51.723Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://www.aricolaprete.com/2026/02/something-big-is-not-happening.html"
source_title: "Something Big Is (not) Happening"
source_id: 47007259
excerpt: "AIはコード生成に強いが、微細な判断と空間表現で致命的な盲点があり人の検証が必須だ"
---

# Something Big Is (Not) Happening - 何か大きなことが（起きているわけではない）
AIは「できること」と「できないこと」の境界線を暴く：プログラミングAIの実力と盲点

## 要約
著者は、生成系LLMがプログラミングでは驚くほど有用だが、細部や空間的・判断的な課題では未だ欠陥があると指摘する。だから「大きな変化」は部分的に起きているが、万能ではない。

## この記事を読むべき理由
日本の開発現場や企業がAI導入を検討する際、何を任せて何を人が残すべきかの判断材料になるため。安全性・法務・高信頼システムを扱う日本企業には特に重要。

## 詳細解説
- コア主張：LLMは「テキストで機械的な正解を出す」タスク（コード生成、要約、情報検索）に強い。出力が期待する結果になれば「動く」と評価されるため、プログラミングとの親和性が高い。  
- 限界：微細な差異の把握（例：文字数や微妙な空間配置、絵文字の判別など）や、重要な判断・決定（法的・生命に関わる意思決定）は苦手。トークン間の関係で空間的情報を表現するのが不得手で、書籍のレイアウト依存表現など再現できない事例がある。  
- 方法論的視点：観察から逆算して物語を作る「事後的推論」に頼りがちで、これは誤った確信（hallucination）につながる。したがってツール化・検証・人間の介入が不可欠。  
- 文化的・理論的参照：著者はカントや特殊な本（例：House of Leaves）を例に、空間的テクスト表現の再現困難さを示す。

## 実践ポイント
- 高リスク領域ではLLMの出力を「提案」として扱い、最終判断は人が行う。  
- コード生成には単体テスト・型チェック・CI検証を必須化して自動化で安全弁を作る。  
- 敵対的テスト（stumpers）を用意してモデルの弱点を定期的に洗い出す。  
- レイアウトや空間情報が重要なデータは、構造化表現（JSON、座標、表形式）や専用ツールで補完する。  
- 日本の法規制・品質基準を踏まえた導入計画（段階的適用、監査ログ、説明責任）を設計する。

短く言えば、AIは既に「できること」を大きく広げたが、「できないこと」を見極め、適切に人とツールを組み合わせる実装力が勝敗を分ける。
