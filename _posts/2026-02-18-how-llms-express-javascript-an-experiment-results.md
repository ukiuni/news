---
layout: post
title: "How LLMs Express JavaScript (an experiment, results inside) - LLMがJavaScriptを「表現する」方法（実験と結果）"
date: 2026-02-18T18:12:38.960Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://terminalvalue.net/"
source_title: "Terminal Value: Approaching LLMs Like An Engineer"
source_id: 438573802
excerpt: "数十〜百MBのミニファイJSをLLMが抽象化し実務で自動修正可能にする実験レポート"
image: "https://terminalvalue.net/images/llms-think-js.png"
---

# How LLMs Express JavaScript (an experiment, results inside) - LLMがJavaScriptを「表現する」方法（実験と結果）
驚くほど巨大なJSファイルを「理解」して修正するAI──あなたのフロントエンド業務を変えるかもしれない実験レポート

## 要約
大型言語モデル（LLM）にコンパイル済みの大量JavaScript（数十〜百MB相当）をそのままコンテキストとして与えると、モデルはコードを抽象的に「概念化」して解析・編集タスクをこなせる。特にLlama-4系やGoogleのGeminiで顕著な挙動が観察された。

## この記事を読むべき理由
日本の多くの企業が大規模なフロントエンド（SPA）を運用しており、コードレビュー・解析・ローカライズ作業の工数削減や品質改善に直結する可能性があるため。LLMを使った新しい実務ワークフローの示唆が得られる。

## 詳細解説
- 実験の概要  
  - 研究者はFacebookのコンパイル済みフロントエンドJSバイナリ群（複数チャンク、合計で数十〜100MB相当）をプロンプトの先頭に大量に入れ、追加の短い指示を与えた。  
  - 対象モデル：Llama-4-Maverick-17B-128E-Instruct-FP8（Together API）、Gemini-3系（Google）。複数の変種・試行で再現性あり。

- 主な観察結果  
  - モデルは巨大なJSコンテキストを読み込み、ページのDOMやスタイルを変更するような具体的タスク（例：index.htmlの背景を赤にする）を一貫して実行できた。  
  - 注目すべきは「入力トークンに対して出力（completion）トークンが非常に小さい」点。大量のコンテキストを内在化した上で、少ない出力で正確な改変を指示する傾向があった。  
  - 著者はこれを「モデルは推論よりも数学的・抽象的表現（＝コードの構造や振る舞い）を学んでいる」と解釈している。トランスフォーマー学習が大量の公開コード（GitHub等）を吸収している点が背景にある。

- 理論的示唆  
  - LLMは言語だけでなく「コードを上位概念で表現」でき、見たことのない抽象的解法を出すことがある。大規模SPAのバイナリやミニファイ済みコードでも、モデルが抽象化して操作可能にする可能性がある。

- 注意点  
  - 成功には「大量のコンテキストの投入」「適切なモデル選定」「明確な指示」が重要。バイナリを入れ忘れると結果が変わる。プライバシー・ライセンス・セキュリティ上の懸念も無視できない。

## 実践ポイント
- 小さく試す：まずは数MB〜数十MBのチャンクで検証。完全バイナリ投入ではなく、関係するコンパイルチャンクを選んで試す。  
- コンテキスト設計：必要なバイナリ／チャンクを先頭に置き、続けて短く明確な指示（例：「index.htmlの背景色を赤に変更して理由を一行で説明」）を与える。  
- モデル選び：大きなコンテキストを扱えるモデル（Llama-4 Maverick系、Gemini系など）を選択。トークン予算と応答トークン量をモニタリングする。  
- 実務応用案：大規模コードベースの要約、影響解析、差分指示による自動修正支援、ローカライズ前の一括確認など。  
- リスク管理：機密・顧客データは投入しない、OSSライセンスとIPの取り扱いを確認、生成結果は必ず人による検査を行う。

この実験は「LLMが巨大なコンパイル済みコードを抽象的に扱える」ことを示唆しており、日本の開発現場でも効率化や新しい自動化の糸口になる可能性が高い。まずは安全な範囲で小さく試してみることを勧める。
