---
layout: post
title: "Why I don't think AGI is imminent - なぜ私は汎用人工知能（AGI）が差し迫っているとは思わないのか"
date: 2026-02-19T01:37:51.550Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://dlants.me/agi-not-imminent.html"
source_title: "Why I don't think AGI is imminent"
source_id: 922646861
excerpt: "言語モデルだけでは感覚と行為が欠け、AGI実現はまだ遠い理由"
---

# Why I don't think AGI is imminent - なぜ私は汎用人工知能（AGI）が差し迫っているとは思わないのか
「言語モデルだけで“人間らしい知能”は作れない」――本当に知っておくべきAIの現状

## 要約
トランスフォーマー型の大規模言語モデル（LLM）は驚異的だが、「数や物の不滅性、因果、空間把握」といった進化的に築かれた認知の基礎（cognitive primitives）が欠けており、観測だけの学習ではそれらを獲得できない可能性が高い――ゆえにAGI到来は楽観視できない、という主張。

## この記事を読むべき理由
日本はロボティクス、製造、自動運転、介護など「身体性（embodiment）」が重要な領域で世界をリードする可能性がある。今のAIがどこまでできて、どこがまだ遠いのかを知ることは、技術投資や政策判断、現場導入の期待値調整に直結します。

## 詳細解説
- 認知的基盤（cognitive primitives）とは：動物に共通する「物の持続性（object permanence）」「数感覚」「因果関係」「運動の生死判別」など。人間の言語理解はこれらを前提に成り立っており、文中に明示されない情報を“裏取り”して解釈する。
- LLMの限界：トランスフォーマーはテキスト統計からパターンを学ぶため、数値の直感や一般的な因果推論、記憶に基づく長期の行動計画などで脆弱。例：多桁算や単純な論理的逆転（AはB → BはA）で失敗しやすい。
- 視覚・動画での学習は部分解決だが不十分：動画予測や大量の合成データで「シェルゲーム」を学ぶモデルは、その特定状況だけを模倣する可能性が高く、本質的な「持続する物体」としてのコミットメントを得たとは言いにくい。
- 「世界モデル」と実装例：DeepMindのSIMA 2（行動模倣中心）、Dreamer 4（世界モデル上でのRL訓練）、Yann LeCunのJEPA（表現空間で予測）などは前進だが、いずれも「観察ベース」か、特定タスク向けの制御性能が主目的で、観察→行動→再観察という密な感覚行動結合から生まれる根源的な認知プリミティブの獲得は未証明。
- ベンチマークが示す差：ENACTやARCといった評価で人間とモデルの差は依然大きく、特に長期的プランニングや記憶、探索が必要な課題で顕著。ARCではリファインメント（生成→検証→修正）を大量に回すことでスコアを上げているが、計算コストが膨大で「核心的な理解」が得られた証拠とは言い難い。
- 結論的示唆：言語事前学習だけで汎用性の高い認知が自然発生するという見立ては楽観的すぎる。知能の基礎が感覚・行為と結びついた長年の進化過程に由来するなら、同様の学習環境（豊富な多感覚データと行為のフィードバック）が不可欠で、研究はまだ初期段階で「何十年単位」の課題になり得る。

## 実践ポイント
- AGI到来を前提にした短期投資や安全軽視は避ける：期待値を現実に合わせる。  
- 産学連携で「感覚—行為」データ基盤を整備する：日本企業はロボット現場から得られるマルチモーダルデータを収集・共有する価値がある。  
- 製品評価にENACT/ARCのような「身体性を問うベンチマーク」を導入する：視覚・操作・長期計画が必要なタスクでモデルを検証する。  
- 実務者へ：現行LLMは言語処理やドキュメント自動化で強力。空間推論や物理操作が必要な機能は専用モジュール（シミュレーション＋制御）で補うこと。  
- 技術発信では「過剰なAGI主張」を疑い、透明なベンチマーク結果とコスト見積を示すこと。

出典：元記事「Why I don't think AGI is imminent」（https://dlants.me/agi-not-imminent.html）を要約・再構成。
