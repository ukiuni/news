---
layout: post
title: "Grok now bans illegal porn generation, after monetizing it - 収益化ののちにGrokが違法ポルノ生成を禁止"
date: 2026-01-15T15:36:32.194Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://appleinsider.com/articles/26/01/15/grok-now-bans-all-undressing-images----where-its-forced-to?utm_source=rss"
source_title: "Has Grok blocked non-consensual images?"
source_id: 426219824
excerpt: "Grokが収益化後に違法実在人物ポルノ生成を部分的に禁止、だが回避可能で対策は不十分"
---

# Grok now bans illegal porn generation, after monetizing it - 収益化ののちにGrokが違法ポルノ生成を禁止
Grokが禁止した「違法な裸体生成」──でも本当に止まったのか？プロフィットから対応までを読み解く

## 要約
X（旧Twitter）のAI「Grok」は、実在人物の無断ヌードや児童ポルノの生成を技術的にブロックすると発表したが、発表には地域限定の例外や「やむを得ず対応している」ことを示す文言が含まれている。

## この記事を読むべき理由
プラットフォームAIが“収益化→深刻な悪用→後追い対策”という経路を辿った事例は、AIサービスの安全設計・規制対応・アプリストア審査のあり方を考える上で重要。日本の開発者や利用者にも直接関係する問題です。

## 詳細解説
- 背景：Grokはユーザーが実写真をもとに「深層偽造（deepfake）」や非同意の性的画像を生成できる状態が確認され、短時間で違法コンテンツが見つかった。問題が表面化した後、当該機能は一時的に有料化されていた。
- 対外的圧力：インドネシアやマレーシアがブロック、英国での検討や米国からの問合せなど各国の対応が相次いだ。AppleやGoogleのアプリ審査が不十分だったと指摘されている。
- Xの対応内容：X側は「技術的対策を実装した」として、実在人物の下着／ビキニなどを含む画像編集を禁止すると公表。これが有料アカウントにも適用されると明言した一方、「違法とされる地域のみでジオブロックする」といった限定的適用の記述がある。
- 技術的側面（考えられる実装）：  
  - ヌード検出モデルで生成要求を拒否するフィルタリング  
  - 元写真と生成画像の類似性検出（本人写真をベースにした生成を検知）  
  - プロンプト内に実在人物名や識別情報があれば拒否  
  - 地理情報（IPやアカウント情報）によるジオブロック  
  ただし、こうした対策は回避され得る（VPNや改変プロンプト、小さな切り抜き等）ため完全ではない。
- 倫理とビジネスのジレンマ：当初の収益化（有料機能化）と、その後の禁止の流れは「利益優先→外部圧力で修正」の典型で、透明性や第三者監査の必要性が浮き彫りになった。

## 実践ポイント
- 一般ユーザー向け:
  - 不審な生成物やアカウントは速やかにプラットフォームへ報告する。スクリーンショットやURLを保存して通報。
  - 家族や子どもが利用する端末にはペアレンタルコントロールを設定する。  
- 開発者／プロダクト担当者向け:
  - 生成AIに対する安全フィルター（ヌード検知、実在人物識別、プロンプト拒否）を設計段階から組み込む。  
  - 有料化の前後で悪用リスク評価と外部監査を行い、透明なポリシーを公開する。  
- 事業者／規制担当者向け:
  - アプリストア審査や国内法との整合性チェックを強化する。必要ならガイドライン改定や違反時の迅速な対応策を整備する。

短く言えば、Grokの対応は一歩ではあるが限定的で、技術的にも運用的にも「完全解決」ではない。日本でも同様の問題は起こり得るため、利用者・開発者・規制側それぞれが前向きに対策を講じる必要がある。
