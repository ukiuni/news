---
layout: post
title: "AI Is Still Easy to Trick • Katharine Jarmul - AIはまだ「だましやすい」• キャサリン・ジャーミュル"
date: 2026-01-08T13:47:41.228Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://youtu.be/lz0L0rRV7RE?list=PLEx5khR4g7PINwOsYrkwz3lTTJUYoXC53"
source_title: "Hacking AI Systems: How to (Still) Trick Artificial Intelligence • Katharine Jarmul • GOTO 2025 - YouTube"
source_id: 468037445
excerpt: "全角・異体字やプロンプト攻撃でAIは簡単に誤動作し、実運用で必須の防御策を具体的に示す"
image: "https://i.ytimg.com/vi/lz0L0rRV7RE/maxresdefault.jpg"
---

# AI Is Still Easy to Trick • Katharine Jarmul - AIはまだ「だましやすい」• キャサリン・ジャーミュル
AIはまだ簡単にだまされる — 現場で知っておくべき攻撃と防御

## 要約
AI（特に大規模モデルや学習済みシステム）は依然として様々な手法で誤作動させられる。攻撃手法の種類と現実的な対策を知っておけば、サービス運用やプロダクト設計で被害を減らせる。

## この記事を読むべき理由
日本企業・開発者がチャットボット、画像認識、自動化サービスを導入する中で、攻撃面（API経由の悪用、入力の細工、学習データ汚染）は現実のリスク。金融、医療、カスタマーサポートなどでの信頼性と法令順守に直結するため、基礎知識と実践的対策が必要になる。

## 詳細解説
- 攻撃の分類  
  - 敵対的摂動（adversarial examples）: 画像や音声にごく小さなノイズを加えるだけで誤判定させる。これらはモデル特性（勾配や境界）を突く。  
  - プロンプトインジェクション／ジャイルブレイク: LLMに対して悪意ある指示を混入して不適切な応答を引き出す。  
  - データ汚染（poisoning）: 学習データに悪意あるサンプルを混ぜ、モデルの挙動を長期的に変える。  
  - モデル抽出・情報漏洩: APIから繰り返し問い合わせをしてモデルを複製したり、訓練データのプライベート情報を推定する（membership inference 等）。  
- 攻撃の性質と「転移性」  
  - あるモデルで作った敵対的例が別モデルでも効くことが多く、攻撃はブラックボックスでも成立しうる。  
- 日本語固有の脆弱性例  
  - トークナイザや正規化の違い（全角/半角、異体字、改行・空白の扱い）でプロンプトインジェクションが容易になる場合がある。  
  - ひらがな／カタカナ変換や語形変化を悪用した回避も想定される。  
- 防御手法（概要）  
  - 入力側の正規化とサニタイズ（Unicode正規化、制御文字除去、同形文字の正規化）  
  - アドバーサリアルトレーニングや堅牢化した学習（敵対的サンプルを混ぜる）  
  - 出力フィルタリング／ポリシーチェックと多段検証（モデル回答を別のモデルで検証）  
  - 差分プライバシーやウォーターマークで訓練データ漏洩やモデル抽出へ対抗  
  - ログ監視、レート制限、リスクベースのアクセス制御（異常なクエリを検出して遮断）  
- 評価とレッドチーミング  
  - 実運用前に攻撃シナリオを想定したテスト（赤チーム演習）を実施することが効果的。ベンチマークでの堅牢性確認も重要。

## 実践ポイント
- 脅威モデルを作る：どの攻撃が自分のサービスに致命的かを明確にする。  
- 入力を必ず正規化する：Unicode正規化（NFC/NFKC）、全角半角統一、不要な制御文字の削除。  
- プロンプトの境界を明確に：ユーザー入力をそのまま実行系に渡さない。テンプレートで命令部分と入力を分離する。  
- レート制限とモニタリング：異常なクエリパターンや高頻度アクセスを検出して遮断する。  
- レッドチームを回す：実際にプロンプトインジェクションやデータ汚染の試作をして脆弱性を洗い出す。  
- 日本語固有のテストを行う：全角/半角、異体字、送り仮名、省略表現などを含む攻撃ケースを用意する。  
- プライバシー保護：訓練データに個人情報が入らないよう差分プライバシーやデータガバナンスを適用する。  

最後に一言：AIは強力だが万能ではない。攻撃シナリオを想定した設計と運用が、信頼できるサービスの鍵になる。
