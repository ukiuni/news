---
layout: post
title: "Researchers extract up to 96% of Harry Potter word-for-word from leading AI models - 生データを丸写し？主要LLMから「ハリー・ポッター」を最大96%抽出した研究"
date: 2026-01-10T16:30:13.018Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://arxiv.org/abs/2601.02671"
source_title: "[2601.02671] Extracting books from production language models"
source_id: 467534151
excerpt: "研究：主要LLMが『ハリー・ポッター』を最大96%丸写し、実運用でも著作権漏洩の危機"
image: "/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

# Researchers extract up to 96% of Harry Potter word-for-word from leading AI models - 生データを丸写し？主要LLMから「ハリー・ポッター」を最大96%抽出した研究
「あなたのサービスのAI、本当に安全ですか？」——主要プロダクションLLMから本がほぼそのまま取り出せる可能性を示す衝撃の報告

## 要約
研究は、商用レベルの大規模言語モデル（LLM）から著作権付き書籍を実際に抽出できるかを検証し、条件次第で最大95.8%の復元（nv-recall）を達成したと報告します。プロダクション環境の安全策があっても、訓練データの「記憶」からの漏出は現実的なリスクです。

## この記事を読むべき理由
日本の企業・開発者が自社サービスでLLMを使う際、知らずに著作権侵害リスクや情報漏洩につながる設定・運用をしている可能性があります。法律・ビジネス面だけでなく、技術的な評価手法と対策を知ることは必須です。

## 詳細解説
- 研究の目的と手順  
  - 目的：LLMが訓練データを重複／丸写し（memorization）しているか、実運用モデルでも抽出可能かを評価する。  
  - 方法：2段階プロシージャを採用（1）抽出の可否を試す初期プローブ（場合によってはBest-of-N（BoN）＝多数生成して最も長く一致する出力を選ぶ“jailbreak”的手法”を利用）、（2）反復的な継続プロンプトで書籍全文の抽出を試行。  
  - モデル：Claude 3.7 Sonnet、GPT-4.1、Gemini 2.5 Pro、Grok 3 の4種で評価。評価指標はブロックベースの最長共通部分列近似「nv-recall」。  

- 主な結果  
  - Gemini 2.5 Pro：BoNやjailbreak不要で比較的高い抽出（例：Harry Potterで nv-recall ≈ 76.8%）。  
  - Grok 3：同様に高い抽出（例：70.3%）。  
  - Claude 3.7 Sonnet：jailbreakを用いると書籍をほぼ丸写しに近い出力（nv-recall ≈ 95.8%）を示した。  
  - GPT-4.1：非常に多数のBoN試行が必要で、最終的には継続を拒否するなど制限され低い復元率（例：4.0%）に留まったが、試行回数の差で挙動が大きく変化。  

- 研究上の注意点  
  - 実験は2025年8月〜9月に実施、90日間の通報猶予の後に公開。  
  - nv-recallは「完全一致率」をそのまま示すものではなく、ブロックごとの一致を基にした近似指標。だが実用的には「長い連続した原文の復元」を可視化するのに有効。

- 意味するところ（技術的・法的含む）  
  - モデル本体（重み）に訓練データが相当量“記憶”され得ること、そして出力層や生成制御だけでは完全に防げない場合があることを示唆。  
  - プロバイダや導入企業は、単なる出力フィルタだけでなく、学習データの管理・匿名化・差分プライバシーなど多層的対策が必要。

## 実践ポイント
- エンジニア向け（すぐにできる技術対策）
  - 社内で使うLLMの出力を定期的に「長文一致検査」する（nv-recall類似のスクリプトで既知の著作物と比較）。  
  - 訓練データに未許諾の著作物が混入していないか、データ収集パイプラインを監査する。  
  - 学習時のデータ重複排除、データ最小化、差分プライバシー（DP）導入、または重みへの記憶を抑える正則化を検討する。  
  - 本番系では出力の長文連続生成に対するレート制限やステップごとの安全チェックを実装する。  

- 法務／プロダクト向け
  - 出力が著作権付きテキストを含むリスクを考慮し、利用規約・コンテンツ方針を整備、外部コンテンツ保有者への開示体制を確立する。  
  - パブリッシャーや顧客との契約で訓練データのソース・ライセンスを明確化する。  

- 日本市場への示唆
  - 日本語・日本市場特有のコンテンツ（商業書籍、雑誌、ユーザー生成コンテンツ）を学習に使う場合、国内出版社や権利者との契約リスクが高い。出版社側もAI時代の権利管理・技術的ガードレール（例：ウォーターマーク、メタデータ管理）を検討する必要がある。  
  - スタートアップは、導入前にモデルの「丸写し脆弱性」を評価するセキュリティテスト（red-team）を必須化することを推奨。

この記事の核心：やや驚くべきことに、運用モデルでも条件次第で長文を丸写しできる場合があり、技術的対策と法的備えの双方を早急に整える必要がある、という点です。
