---
layout: post
title: "Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents? - AGENTS.mdの評価：リポジトリ単位のコンテキストファイルはコーディングエージェントに有効か？"
date: 2026-02-17T07:52:21.997Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://arxiv.org/abs/2602.11988"
source_title: "[2602.11988] Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?"
source_id: 47034087
excerpt: "AGENTS.mdは成功率を下げ推論コスト20%以上増、必須要件に限定せよ"
image: "/static/browse/0.3.4/images/arxiv-logo-fb.png"
---

# Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents? - AGENTS.mdの評価：リポジトリ単位のコンテキストファイルはコーディングエージェントに有効か？
魅力的な日本語タイトル: リポジトリの「AGENTS.md」は本当に役立つのか？― コード支援AIに効く/効かない意外な実験結果

## 要約
リポジトリ固有のコンテキストファイル（例：AGENTS.md）を与える運用が広がる中、複数のコーディングエージェントとLLMで評価した結果、コンテキストファイルは成功率を下げ（＝タスク達成を阻害することが多く）、推論コストを20%以上増やす傾向があった。人間作成・LLM生成いずれでも、指示はエージェントの探索行動を広げるが、不要な要件が混じると逆効果になる。

## この記事を読むべき理由
日本の企業・開発チームでもリポジトリ単位でAIをチューニングする実践が増加中。コストや品質に直結する指針なので、無闇にAGENTS.mdを増やす前に知っておくべきエビデンスが示された。

## 詳細解説
- 研究の枠組み：2つの補完的な設定で評価（1）既存のSWE-benchタスクに対して、エージェント開発者の推奨に従いLLMが生成したコンテキストファイルを付与、（2）実際に開発者がコミットしたコンテキストを含むリポジトリからの新規Issue群で検証。
- 評価対象：複数のコーディングエージェントと言語モデルを横断的に評価し、タスク成功率・探索行動・推論コストを計測。
- 主な発見：
  - コンテキストファイルを与えるとタスク成功率が低下する傾向（nullより悪い場合が多い）。
  - 推論コスト（トークンや呼び出し回数ベース）は20%超増加。
  - 行動変化：より広い探索（詳細なテスト実行、ファイル横断など）を促進し、エージェントはファイル内指示を遵守する。
  - 解釈：不要な要件や過度に詳細な指示が探索を拡散させ、実務タスクを難しくする。
- 結論：コンテキストは「質が重要」で、特に人間が書く場合は最小限の必須要件に留めるべき。

## 実践ポイント
- AGENTS.mdを導入する前にA/Bテストを実施（with/without、複数モデルで比較）。
- ファイルは「必須の前提」だけに限定：不要な制約や詳細手順は排除する。
- コスト計測を取り入れる：推論トークン数・API呼出回数で運用コストを見える化。
- 自動生成する場合はテンプレートを厳格にし、人的レビューを必須化。
- 日本の現場ではセキュリティ/社内ポリシー要件だけ明確に記載し、実装方針や非必須の慣習は別ドキュメントへ分離する。

論文の示した「不要な要求は邪魔になる」点は、AIを実運用に組み込む日本の現場でもすぐに試せる有益な示唆である。
