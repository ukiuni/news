---
layout: post
title: "Per-query energy consumption of LLMs - LLMのクエリ当たりのエネルギー消費"
date: 2026-01-08T13:46:53.381Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://muxup.com/2026q1/per-query-energy-consumption-of-llms"
source_title: "Per-query energy consumption of LLMs - Muxup"
source_id: 701298253
excerpt: "公開ベンチからLLMのWh/クエリを推定し日本の電力・CO2影響を可視化"
image: "https://v1.screenshot.11ty.dev/https%3A%2F%2Fmuxup.com%2F2026q1%2Fper-query-energy-consumption-of-llms/opengraph/ar/bigger"
---

# Per-query energy consumption of LLMs - LLMのクエリ当たりのエネルギー消費
魅惑のタイトル: 「あなたの1リクエストはどれだけ電気を食うのか？〜LLMの“Wh/Query”を見える化する方法と日本での意味」

## 要約
オープンウェイトモデルを対象に、公開ベンチマーク（InferenceMAX）の結果から「クエリ1回あたりの消費電力（Wh/クエリ）」を推定する試みと、その限界・注意点を解説する。

## この記事を読むべき理由
API料金や利用量の増加が話題になる中で、「使うほど環境負荷は増えるのか」「本当に安いのか」を定量的に把握できる知見は、日本の企業や開発者がコスト・責任を判断する際に直接役立つから。

## 詳細解説
- 対象とデータ源：著者は InferenceMAX ベンチマークの公開結果を利用。代表モデルとして DeepSeek‑R1‑0528（671Bパラメータ、37Bのactive weights相当で大規模MoEの例）と gpt‑oss‑120b を比較対象とした。
- ベンチの設計ポイント：
  - 入出力長の組合せ（ISL/OSL）を 1k/1k、1k/8k、8k/1k といった現実的なパターンで評価。
  - GPU種別（単一〜大規模クラスター）や量子化設定（fp8, fp4）などの幅広い条件で測定。
- ベンチ結果をWh/クエリに変換する際の注意点（重要）：
  - 実測長さと設定のズレ：設定パラメータ --random-range-ratio がデフォルト0.8で、たとえば「8192（8k）」指定でも平均は約0.9×8192≒7373トークンとなる。記載の“8k”は実際より短いワークロードを示す可能性がある。
  - スループット定義：元は「生成トークン」意図だったが、実際には入力＋出力トークン合計で算出している点に注意（修正あり）。
  - 失敗リクエストの扱い：HTTP 200 を返さないリクエストは再試行せず失敗として集計から除外されるため、過負荷時に「拒否して短いレイテンシ」を出すような挙動がベンチに有利に働く可能性がある。
  - ignore_eos の使用：出力がEOS（終端）を無視して続く設定が使われているため、ランダム入力でループし予測しやすい継続出力が増え、スループット改善に寄与するリスクがある。
  - 分散（disaggregated）構成の比較問題：prefill/decodeを分離した構成は、GPUあたりのスループット比較が単純にはできない（平均化が必要）。
- エネルギー推定の考え方：
  - 「プロバイダがトークン単価で電気代以下で販売している」は現実的でない（電力コストが下限になるという上限論的議論）。
  - 実データが得られれば、クエリ長分布、総リクエスト数、システム実測消費電力（W）から Wh/クエリ を直接計算できる。

数式（Wh/クエリの基本式）：
$$
\text{Wh/query} = \frac{P\;(\text{W}) \times T\;(\text{h})}{N}
$$
あるいは秒単位で測る場合は
$$
\text{Wh/query} = \frac{P\;(\text{W}) \times T\;(\text{s})}{3600 \times N}
$$

例：クラスタの平均消費が10,000Wで1時間に10,000クエリ処理したら
$$
\text{Wh/query} = \frac{10{,}000 \times 1}{10{,}000} = 1\;\text{Wh}
$$

## 実践ポイント
- APIを選ぶとき：料金だけでなく「モデルの効率（小さいモデル＝低Wh/クエリ）」「平均トークン数」を確認すると良い。短いプロンプト＋出力で済むなら小型モデルで十分なことが多い。
- 自分で測る方法（簡易）：
  1. 可能なら実運用期間の総消費電力（Pの平均）を取得。
  2. 同期間の総リクエスト数とトークン長の分布を取得。
  3. 上記式で Wh/クエリ を算出し、トークン長で正規化する（Wh/1k token 等に変換）。
- 開発側での対策：キャッシュ、バッチ化、生成長の上限、低精度量子化（fp8/fp4）や小型モデルの活用で消費電力を下げられる。
- 透明性要求：プロバイダ／ホストには「期間中の総リクエスト数・長さ分布・実測システム消費電力」を公開するよう求めると、より比較可能なエネルギー指標が得られる。
- 日本市場的視点：電力単価・電力のカーボンインテンシティ（地域差）が大きく影響するため、日本国内/海外リージョンの選定で実効的なCO2排出量が変わる点を意識する。

この記事で得られる価値は、「ベンチマーク結果をそのまま鵜呑みにしない目」と「自分でWh/クエリを推定し実運用に落とし込むための具体的手順」です。必要なら、InferenceMAXのJSONからWh/クエリを算出する簡単なスクリプト例も用意できます。要望があればお知らせください。
