---
layout: post
title: "Former OpenAI policy chief creates nonprofit institute, calls for independent safety audits of frontier AI models - 元OpenAIポリシー責任者が非営利機関を設立、フロンティアAIモデルの独立安全監査を要求"
date: 2026-01-20T22:37:47.394Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://fortune.com/2026/01/15/former-openai-policy-chief-creates-nonprofit-institute-calls-for-independent-safety-audits-of-frontier-ai-models/"
source_title: "Exclusive: Former OpenAI policy chief debuts institute, calls for independent AI safety audits | Fortune"
source_id: 423150395
excerpt: "元OpenAI幹部がAVERI設立、フロンティアAIに独立監査を義務付ける動き"
image: "https://fortune.com/img-assets/wp-content/uploads/2026/01/Miles-Headshot-e1768432371276.jpeg?resize=1200,600"
---

# Former OpenAI policy chief creates nonprofit institute, calls for independent safety audits of frontier AI models - 元OpenAIポリシー責任者が非営利機関を設立、フロンティアAIモデルの独立安全監査を要求
魅力的タイトル: 「AIベンダーに自分で採点させない──元OpenAI研究者が立ち上げた『独立監査』の現実味」

## 要約
元OpenAIの政策研究者マイルズ・ブランデージが、フロンティアAI（最先端モデル）に対する第三者監査を推進する非営利組織AVERIを立ち上げ、監査基準や「AI保証レベル」フレームワークを提案しました。

## この記事を読むべき理由
企業や投資家、保険会社、そして日本の開発チームにとって、AIのリスクを客観的に評価する仕組みが今後のビジネスや規制対応で鍵になります。特に海外で監査基準が動き出す中、日本企業も対策を急ぐ必要があります。

## 詳細解説
- 新組織と目的: AVERI（AI Verification and Evaluation Research Institute）は、AI企業自身による自己評価に依存せず、外部の独立した監査を普及・標準化することを目的としたシンクタンクです。自ら監査を行うのではなく、監査基準や政策提言を作ります。
- 研究論文とフレームワーク: 公開された論文は、監査の実務設計を示し、AIの「保証レベル（AI Assurance Levels）」を1〜4まで定義。レベル1は現状の限定的な第三者テストに相当し、レベル4は国際条約レベルの「治療（treaty）対応」まで想定した最高水準です。
- 監査が求められる背景: 現在、多くの大手ラボは自己報告や外部「レッドチーム」との共同検証に頼るが、評価基準や報告フォーマットはまちまちで、利用者や規制当局が信頼できる第三者保証が不足している点が問題視されています。
- 誘発要因（ビジネス的圧力）: 企業顧客が安全性証明を契約条件にすること、保険会社が保険引受の条件として監査を要求すること、投資家が投資前のリスク評価として監査を求めることなどが、監査普及の現実的なドライバーとされています。
- 人材と実行の課題: AI監査は技術的専門性とガバナンス知見の両方を要するため、人材不足と利害の衝突（監査対象に高報酬で引き抜かれる等）が課題。AVERIは既存の会計・検査会社やサイバー攻撃テスト会社、学術・NPO人材を組み合わせるチームを想定しています。
- 国際規制との接点: 米国は連邦レベルのAI規制が未整備だが、EUのAI法や付随する実践指針は外部評価や適合性評価を想定する箇所があり、欧州域内で外部評価の要件が現実化しつつあります。

## 実践ポイント
- ベンダー選定時に「独立監査の実施有無」と報告書の開示範囲を確認する。契約に監査条件を入れることを検討する。
- 保険や投資を受ける側・行う側は監査の有無をリスク評価項目に加え、将来的に監査基準が標準化されれば条項更新を見越す。
- 日本のプロダクトチームは、モデル公開・運用前に社内で第三者相当のレビューを行い、監査で求められそうな証跡（テスト結果、対策ログ、データ使用履歴）を整備しておく。
- 人材面では、監査に必要な「技術 × ガバナンス」スキルを育成するため、社内研修や外部連携（大学・NPO・監査事務所）を早めに始める。
- 欧州での規制（EU AI Act）やAVERIの動きを追い、コンプライアンス要求が日本の顧客や海外展開先で強まる前に準備する。

短めに言うと、独立監査は今後のAIビジネスの信頼性と資本市場での安全弁になる可能性が高く、日本の企業や開発者も「監査を受ける側／準備する側」として早めに動く価値があります。
