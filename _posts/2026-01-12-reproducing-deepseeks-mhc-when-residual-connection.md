---
layout: post
title: "Reproducing DeepSeek's MHC: When Residual Connections Explode - DeepSeekのmHC再現：残差接続が暴走するとき"
date: 2026-01-12T15:11:15.724Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://taylorkolasinski.com/notes/mhc-reproduction/"
source_title: "DeepSeek&#39;s mHC: When Residual Connections Explode - Taylor Kolasinski"
source_id: 46588572
excerpt: "残差接続の信号爆発をSinkhorn双確率制約で抑える実践的手法"
image: "https://taylorkolasinski.com/og-image.png"
---

# Reproducing DeepSeek's MHC: When Residual Connections Explode - DeepSeekのmHC再現：残差接続が暴走するとき
GPT時代の「信号爆発」を防ぐ。超大規模モデルで致命的になる、残差接続の“増幅”問題をどう抑えるか？

## 要約
DeepSeekが提案する「manifold-constrained Hyper-Connections（mHC）」は、並列ストリームを自由に混ぜると起きる信号の指数的増幅（爆発）を、行列を双確率行列（doubly stochastic）に制約することで防ぐ手法。小規模では表現力を犠牲にするが、大規模では安定性のために必須になる。

## この記事を読むべき理由
日本の研究・企業ベースで大規模モデル（数十億〜数百億パラメータ）を扱う場合、訓練中に起きる「微小な増幅の累積」が致命的なNaNや学習崩壊につながる。設計の選択（より表現力のある構造を許すか、安定性を取るか）が実運用に直結するため、その現象と対処法を理解しておくことは必須。

## 詳細解説
- 標準残差接続の役割  
  通常のトランスフォーマーやResNet系では各層で
  $x_{l+1}=x_l+F(x_l)$
  として入力ストリームを「保存しつつ」層の更新を足す。これが勾配の経路を保ち、深いネットワークを安定させる“保存則”の働きをする。

- Hyper-Connections（HC）のアイデア  
  1本の流れを$n$本の並列ストリームに拡張し、学習可能な混合行列でストリーム間を行き来させる：
  $$
  x_{l+1}=H^{res}_l x_l + H^{post,T}_l\;F(H^{pre}_l x_l, W_l)
  $$
  ここで $H^{res},H^{pre},H^{post}$ がストリームのルーティングを司る。理論的には表現力が増すが、これらが「無制約」だと信号を増幅してしまう危険がある。

- 増幅量の指標（Amax）  
  行列が信号をどれだけ増幅できるかは
  $$
  A_{\max}(M)=\max\Big(\max_i\sum_j|M_{ij}|,\ \max_j\sum_i|M_{ij}|\Big)
  $$
  で評価でき、無制約のHCでは層ごとの小さな増幅が重なって数百倍〜数千倍に達する（論文では27Bで最大3000x報告）。

- mHC（manifold-constrained HC）の対処法  
  増幅を抑えるために、主要な混合行列（特に累積誤差を生む $H^{res}$）を双確率行列にプロジェクションする。双確率行列は
  - 要素が非負
  - 各行の和が1
  - 各列の和が1
 であり、加重平均しかできないため増幅は起きない。

- Sinkhorn-Knoppでの実装  
  具体的には学習パラメータ $H$ から次を実行：
  1. $P=\exp(H)$（全要素を正に）  
  2. 行正規化（各行を足して1に）  
  3. 列正規化（各列を足して1に）  
  4. 2–3を数回繰り返す（通常20回程度で収束）  
  これを差分可能にして逆伝播を通すことで、モデルは生の重みを学習しつつ、実際に用いる混合行列は常に双確率行列になる。

- 再現実験の要点（Taylorのノート）  
  - データ：TinyShakespeare、モデルはGPT-2風、約10Mパラメータ  
  - 結果：小規模では無制約HCの方が平均検証損失は良い（より表現力）が、種（seed）によるばらつきが大きく、Amaxが6〜9xと不安定。mHCは損失はやや劣るがAmaxは常に1.0で完全に安定。大規模（論文の27B）ではHCは3000xに達して学習が崩壊するため、mHCの制約が実用上必要。

## 日本市場との関連性
- 多くの日本企業や研究機関が「中規模から大規模」へモデルを拡張中。小さな実験では見えない爆発的な不安定性が、GPUクラスタやコスト運用で致命的になる。  
- 日本のプロダクション環境では再現性・安定性が事業継続性に直結するため、表現力を追うだけでなくこうした設計上の安全弁を組み込む判断が重要。  
- Apple Mシリーズでの再現が報告されている点は、ローカルでのプロトタイプ検証が比較的容易であることを示唆。大規模試験はA100などのインフラで行う必要あり。

## 実践ポイント
- まずは監視：訓練中に $A_{\max}$ をモニタリングし、1→数倍の増幅がないか確認する。  
- 主要対策：累積的に作用する混合行列（例：$H^{res}$）にはSinkhornで双確率制約を課す。$H^{pre}$/$H^{post}$はシグモイド等でバウンドするだけでも効果あり。  
- 学習率の扱い：HCで不安定な挙動が出るなら学習率を下げるが、根本解決は行列の制約。学習率は一時的な対症療法と考える。  
- 小規模でのトレードオフを承知する：10M規模ではmHCは表現力に対する“安定化コスト”を払うが、27B規模ではそのコストが破滅を防ぐ。実運用向けモデルでは安定性を優先することを推奨。  
- 実装上の注意：Sinkhornの反復回数（例20）や初期化、指数化（exp）のスケーリングにより収束特性が変わるので、検証とチューニングを行う。  
- 再現を重視するなら：複数シードでの評価、深さ（層数）スイープ、幅の調整などを恒常的に行い、ばらつきが許容範囲か確かめる。

以上がTaylor Kolasinskiの再現ノートが示す要旨。次は大規模での「どこで壊れるか」を示す続編（Part 2）が予定されており、実運用での設計判断にとって重要な続報となる。
