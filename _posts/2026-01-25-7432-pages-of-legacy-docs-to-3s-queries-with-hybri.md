---
layout: post
title: "7,432 pages of legacy docs to 3s queries with hybrid search + reranking - レガシー文書7,432ページを3秒で検索する、ハイブリッド検索＋再ランキングの実装"
date: 2026-01-25T04:11:47.924Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://clouatre.ca/posts/rag-legacy-systems/"
source_title: "RAG for Legacy Systems: 7,432 Pages to 3s Answers | Hugues Clouâtre"
source_id: 419449328
excerpt: "7,432ページのレガシー文書をハイブリッド検索＋再ランキングで3秒応答、ROIは1日で回収"
image: "https://clouatre.ca/posts/rag-legacy-systems/index.png"
---

# 7,432 pages of legacy docs to 3s queries with hybrid search + reranking - レガシー文書7,432ページを3秒で検索する、ハイブリッド検索＋再ランキングの実装
レガシーPDF山積みを“一日で回収”する──RAGとハイブリッド検索で現場がすぐ使えるナレッジ検索に

## 要約
20年分・7,432ページのPDFドキュメントを、Retrieval-Augmented Generation (RAG) とハイブリッド検索（BM25＋ベクトル）＋クロスエンコーダ再ランキングでクエリ応答3–5秒にした実運用事例。初期構築170秒、キャッシュ後は2秒で更新、ROIは1日で回収。

## この記事を読むべき理由
- 日本の現場でも、古いPDFマニュアルやオンプレ仕様書が手作業検索の時間泥棒になっています。本手法は短期間で現場検索を劇的に改善し、準拠性（出典の提示）も保てます。
- モデル固有の再学習を不要にするため、運用コストとガバナンスのハードルが下がります。

## 詳細解説
- なぜRAGを選ぶか  
  - ファインチューニングは「知識を重みへ焼き付ける」ため、更新ごとに再学習が必要／出典検証が困難。RAGは元文書を検索してLLMにその場で答えさせるため、更新が数秒で反映・出典提示が可能。低頻度クエリ（週数十件）や継続的に変わるドキュメントには最適。

- 取り込みパイプライン（6段階）  
  1. Extract: PyMuPDFでPDF→テキスト（約44ページ/秒）  
  2. Transform: Markdown化して見出しを復元  
  3. Chunk: 見出し単位で意味の切れ目を保ちつつ分割（1,000字上限・200字重なり）  
  4. Embed: ローカルでall-MiniLM-L6-v2等を使いベクトル化（軽量、CPU可）  
  5. Index: ChromaDBなどへ保存  
  6. Retrieve: BM25（キーワード）＋ベクトル（意味）をRRFで融合 → 上位候補をクロスエンコーダ（FlashRank等）で再ランキングして精度を高める

- ハイブリッド検索の意義  
  - キーワード（例: "port 5432"）を漏らさないBM25と、意図（例: "認証の設定方法"）を拾うベクトル検索を合成。Reciprocal Rank Fusion (RRF) で両方の利点を統合すると安定性が高まる。

- 再ランキング（Reranking）  
  - ハイブリッドで拾った候補16件をクロスエンコーダで精査し上位8件を採用。再ランキングの追加コストは数十ms（例: 31ms）で、最終精度を6–8%改善する効果が確認されている。

- ローカル埋め込みとモデル非依存設計  
  - 埋め込みはローカルで十分高速かつ無料。生成はBedrockやAzure/OpenRouter等、プロバイダを差し替えても挙動が安定（再ランキングがレイテンシ安定の鍵）。

- 実測値（要旨）  
  - ドキュメント: 14 PDF／7,432ページ → 20,679チャンク  
  - 初回インジェスト: 170s（抽出120s＋分割20s＋埋め込み25s＋索引用5s）  
  - キャッシュ後: ≈2.2s  
  - 平均クエリ応答: 3–5s（検索80ms＋生成4s＋オーバーヘッド200ms）  
  - コスト目安: $0.01–0.05 / query（ベッドロック例）、典型は入力2,000トークン・出力500トークンで $0.0011（記事内計算例）  
  - 成功率: 種類によるが概ね85–90%が自動解決、10–15%は人手確認が必要

- 失敗パターンと対策  
  - 幻想（Hallucination）: 出典を必ず添えてユーザに検証させる  
  - コンテキストオーバーフロー: 問いを分割、マルチホップ検索導入  
  - データの陳腐化: ハッシュ/タイムスタンプで自動再インデックス化  
  - コーパスの欠落: ドキュメント拡張が必要（システム問題ではない）

- プロトタイプ→本番移行の手順  
  1. 小さな文書セットでPoC（20–30問で検証）  
  2. OpenRouter等の無料枠で品質確認、問題なければBedrock等へ切替（認証・ガバナンス）  
  3. モニタリング（問い合わせ数、受け入れ率、参照ドキュメント）で改善を反復

## 実践ポイント
- まず高インパクト領域（オンボーディング、コンプライアンス、エラーメッセージ）1セットで試す。  
- ローカル埋め込み（all-MiniLM系）でコストゼロ検証→成功ならクラウド生成へ移行。  
- ハイブリッド検索＋RRF＋クロスエンコーダ再ランキングを必須にする（精度が段違い）。  
- 回答には必ず出典（文書名＋ページ）を添え、ユーザに検証させるUIを作る。  
- キャッシュと自動再インデックス（ファイル更新トリガ）で「更新即反映」を確保する。

補足（初心者向けコードの一例）

```python
# python
import fitz  # PyMuPDF
doc = fitz.open("manual.pdf")
for p in range(len(doc)):
    text = doc[p].get_text()
    # 簡易見出し検出（例）
    for line in text.split("\n"):
        if line.startswith("Chapter ") and "." in line:
            print("##", line)
doc.close()
```

短時間で大きな現場改善が見込める手法です。まず一軍のPDF数本で試して、検索レスポンスと出典提示が現場に効くか検証してみてください。
