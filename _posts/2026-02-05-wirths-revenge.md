---
layout: post
title: "Wirth's Revenge - ワースの復讐"
date: 2026-02-05T08:17:43.251Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://jmoiron.net/blog/wirths-revenge/"
source_title: "Wirth&#39;s Revenge"
source_id: 46895381
excerpt: "クラウドとLLMの便利さが肥大化を招き、計測と設計で遅延・コストを抑える術を示す"
image: "https://jmoiron.net/i/297.jpg"
---

# Wirth's Revenge - ワースの復讐
Wirthの逆襲：ソフト肥大化が日本の開発を蝕む理由

## 要約
ソフトウェアは機能や利便性を増す一方で肥大化し、ハードの高速化以上に「遅く」「高コスト」になり得る──Wirthの警告はクラウドとLLM時代に再燃している。

## この記事を読むべき理由
日本のプロダクトでもクラウド依存やAI導入が進む中、性能・運用コスト・ユーザー体験を損なわずに実装するための「落とし穴」と実務的な回避策を知る必要があるから。

## 詳細解説
- Wirthの主張（Wirth's Law）: ソフトはハードよりも早く遅くなっているという観察。ユーザー向けの利便性は増えたが、実行コストやレイテンシを無視すると実用性が低下する。  
- レイテンシの実測例: Dan Luuのキーボード入力遅延測定は、1983年のApple IIeが最小遅延だったことを示した。現代の入力パイプラインは柔軟性を得る代わりに多層化し、遅延を生む。  
- クラウド化のトレードオフ: AWSのようなPaaS/IaaSは初期投資や運用リスクを下げ、スケールを容易にしたが、各抽象化レイヤー（EC2→S3→RDS→Lambda→IAM等）は便利の代償として追加コストと複雑性をもたらす。  
- 実例—ORMのN+1問題: テンプレートで無自覚に外部参照を繰り返すと数十万のクエリを発生させる（テンプレートレンダでDBを焼く事例）。単純な解はプリフェッチ／結合だが、実務ではキャッシュなどの追加複雑化で対応するケースが多い。  
- LLMの罠: LLMは計算コストが高く、繰り返し作業を直接投げると遅く高額になる。正攻法は「LLMにスクリプトを作らせ、そのスクリプトを実行する」ことで、再現性のある決定論的処理に落とすこと。エージェント誤用で不要な心拍（heartbeat）を延々回し課金が雪だるまになる事例もある。  
- スキル喪失の懸念: 研究でAI利用が概念的理解やデバッグ力を損なう可能性が示唆されている。AIを「万能の答え箱」にするのではなく、学習・検証を並行すべき。  
- 理論的メタファー: Busy Beaver関数 $BB(N)$（状態数 $N$ の停止するチューリング機の最大ステップ数）は非計算的で急成長する例。これは「小さな仕様/実装の違いが極端に大きな影響を与えうる」ことの比喩になる。例: $BB(1)=1,\; BB(2)=6,\; BB(3)=21,\; BB(4)=107,\; BB(5)=47,176,870$。

## 実践ポイント
- プロファイル優先：まず計測（レイテンシ／クエリ数／コスト）して問題を定量化する。  
- N+1 に注意：ORM使用時はプリフェッチ（select_related/eager loading 等）を徹底する。  
- キャッシュは設計で：簡易なキャッシュで済むなら導入、それでも整合性とコストを評価する。  
- LLM活用法を分離：反復タスクは「LLMが生成した決定論的スクリプト」を実行する設計にする。  
- コスト監視：クラウド・API・LLMの利用料をアラート化し、意図せぬループ課金を防ぐ。  
- 技術学習を続ける：AIは補助に留め、根本的なアルゴリズムやデバッグ力を維持する文化を作る。  
- 日本市場での応用：モバイルUXや通信コストを意識し、国内のクラウド（リージョン）選定やオンプレ併用でレイテンシと費用を最適化する。

短く言えば、「便利さ」を享受しつつも、計測・設計・学習を怠らないことが現代のWirthの教訓です。
