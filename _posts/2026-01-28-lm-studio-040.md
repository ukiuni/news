---
layout: post
title: "LM Studio 0.4.0 - LM Studio 0.4.0の紹介"
date: 2026-01-28T20:07:25.451Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://lmstudio.ai/blog/0.4.0"
source_title: "Introducing LM Studio 0.4.0 | LM Studio Blog"
source_id: 46799477
excerpt: "llmsterと連続バッチでローカル高スループット運用を可能にするLM Studio 0.4.0"
image: "https://files.lmstudio.ai/0.4.0.png"
---

# LM Studio 0.4.0 - LM Studio 0.4.0の紹介
魅力的なローカルLLM運用へ――GUIに依存しない「llmster」と並列推論で現場が変わる

## 要約
LM Studio 0.4.0は、GUIから分離したデーモン「llmster」によるヘッドレス展開、並列リクエスト（continuous batching）対応、状態保持型の新REST API (/v1/chat)、CLI強化、UI刷新を主軸にしたメジャーアップデートです。

## この記事を読むべき理由
日本企業や開発チームがローカルで高性能なLLMを運用する際、セキュリティ／コスト／CI連携の観点でGUIに依存しないデプロイと高スループット化は即効性のある改善になります。本アップデートはその実装を現実的にします。

## 詳細解説
- llmster（ヘッドレスデーモン）
  - LM StudioのコアをGUIから切り離し、サーバ／クラウド／CI／GPUラック／Colabで単独稼働可能に。CIや自動化ワークフローでのモデル管理・推論が容易に。
  - 例：インストール、起動コマンド
    ```bash
    # bash
    curl -fsSL https://lmstudio.ai/install.sh | bash

    # PowerShell (Windows)
    irm https://lmstudio.ai/install.ps1 | iex
    ```
    起動例:
    ```bash
    # bash
    lms daemon up
    lms get <model>
    lms server start
    lms chat
    ```

- 並列推論（continuous batching）
  - llama.cpp 2.0ベースで、同一モデルへの複数同時推論を連続バッチ処理で実行。従来の逐次キュー待ちを削減しスループット向上。
  - モデルローダに「Max Concurrent Predictions」と「Unified KV Cache」オプション：並列数上限と、メモリの事前割当を共有して可変サイズリクエストを許容。
  - 現状は llama.cpp エンジンで有効、MLX対応は順次。

- 新しい状態ful REST API: /v1/chat
  - ステートを保持するチャットAPI。レスポンスに返る response_id を次回の previous_response_id に渡すことで会話履歴を小さく保ちながら継続可能。
  - レスポンスにはトークン数や速度、TTFT（time to first token）など詳細な実行統計が含まれ、性能チューニングに有用。
  - ローカルMCP（ツール）連携を許可でき、権限は「Permission Keys」で管理。

- CLIとUXの強化
  - 新しい lms chat によるターミナル対話、モデルダウンロード等のCLI中心ワークフロー。
  - UI刷新：チャットのPDF/Markdown/テキストエクスポート、Split View（複数チャット並列）、Developer Mode、アプリ内ドキュメントなど。

## 実践ポイント
- まずはllmsterをサーバやCI環境に導入して、GUI不要のデプロイを試す。
- スループット改善にはモデルロード時に Max Concurrent Predictions を設定し、Unified KV Cache を有効のままベンチマークする（デフォルトは並列4）。
- /v1/chatを使って状態付き会話フローを実装し、response_id を利用して会話を小さく保つ。ツール呼び出しが必要な場合はPermission Keysでアクセス制御を設定。
- 開発／運用チームは lms chat を使ったCLIワークフローを試験導入し、ログやレスポンス統計で負荷設定を調整する。
- 日本の規制や社内情報管理が厳しい場面では、ローカルデーモン運用とPermission Keysの組合せが有効。

短くまとめると、LM Studio 0.4.0は「ローカル／サーバ環境での実運用」を意識したアップデートです。まずはllmsterの導入と /v1/chat を使った小さなプロトタイプから始めることをおすすめします。
