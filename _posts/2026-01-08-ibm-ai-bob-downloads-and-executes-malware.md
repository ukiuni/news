---
layout: post
title: "IBM AI ('Bob') Downloads and Executes Malware - IBMのAI「Bob」がマルウェアをダウンロードして実行"
date: 2026-01-08T18:40:06.821Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://www.promptarmor.com/resources/ibm-ai-(-bob-)-downloads-and-executes-malware"
source_title: "IBM AI ('Bob') Downloads and Executes Malware"
source_id: 46544454
excerpt: "IBMのAI『Bob』の自動実行バグでマルウェア実行やゼロクリック機密流出が可能"
image: "https://framerusercontent.com/images/goGqPVHqyuybIHtuO6dx47fyc.png?width=1600&amp;height=887"
---

# IBM AI ('Bob') Downloads and Executes Malware - IBMのAI「Bob」がマルウェアをダウンロードして実行
魅力的なタイトル: 「あなたのローカルPCを乗っ取る“AIヘルパー”――IBMのBobで見つかった致命的な自動実行バグとは」

## 要約
IBMのコード支援AI「Bob（クローズドβ）」のCLI/IDEに、ユーザー承認をすり抜けて外部スクリプトをダウンロード・実行させる脆弱性が見つかりました。さらにIDE側では「ゼロクリックでのデータ送信」を可能にする既知の流出ベクターも確認されています。

## この記事を読むべき理由
AI支援開発ツールは日本のスタートアップから大企業の開発現場まで急速に導入が進んでいます。もし「常に許可する（auto-approve）」等の設定を使っていると、意図せずマルウェア実行や機密データ流出につながるリスクが現実にあることを理解しておく必要があります。

## 詳細解説
- 背景（AIコーディングエージェントとは）
  - Bobはターミナル（Bob CLI）やAI編集器（Bob IDE）を通じてコード補助やコマンド実行を行うエージェントです。こうしたツールは開発効率を上げますが、実行権限を持つ分だけ安全設計が重要です。

- 攻撃の流れ（攻撃チェーン）
  1. 利用者が新しいリポジトリを調べるためBobに手伝いを求める。
  2. README等に仕込まれた指示によりBobは一連の「安全そうな」echoコマンドなどを提示する。利用者が3回目に「always allow（常に許可）」を選ぶ。
  3. 攻撃者が用意した悪意あるコマンドが、あらかじめ許可した「echo」で始まる形に加工されているため、承認ダイアログや解析ロジックをすり抜けて実行される。
  4. 結果として攻撃者のサーバーからスクリプトが取得・実行され、ランサム、リバースシェル、情報窃取、仮想通貨マイニング等の被害を招く可能性が生じる。

- 技術的原因（なぜバイパスできたのか）
  - マルチパートコマンド検出で「;」や「&&」といった分離は扱われるが、リダイレクトやプロセス代入の扱いに抜けがあった。
  - Bobの検査関数はコマンド代入 $(...) やバックティック `...` の検出を行うが、プロセス代入 >(command) を検出対象から漏らしていた。
  - そのため、攻撃者はプロセス代入やリダイレクトを用いて「echo 無害 | >(悪意のあるコマンドの出力を実行する)」のような形で本来分離されるべきサブコマンドを隠蔽できた。
  - また、承認モーダルがチェーン内のすべてのサブコマンドを列挙せず「echo」だけを表示する場合があり、利用者が誤って全体を許可してしまう設計的問題がある。

- IDE側のデータ流出（ゼロクリックのリスク）
  - モデル出力に埋め込まれるMarkdown画像やMermaid図で外部URLへのリクエストが発生し、CSP（Content Security Policy）が外部ストレージ（例: storage.googleapis.com）を許可しているため、攻撃者はURLに機密情報を埋めてアクセスログで受け取れる。
  - さらにJSONスキーマの事前取得（pre-fetch）で外部URLが評価され、ファイル編集を承認する前にデータが送信されるケースがある。これらは「ユーザーが何もクリックしなくても」データが抜かれるゼロクリック流出ベクターです。

## 実践ポイント
- 設定と運用
  - CLIやエージェントで「常に許可（auto-approve）」を絶対に使わない。必要最小限のホワイトリストを作る。
  - ワイルドカード（*）や幅広い許可設定は避ける。明示的なコマンド名＋引数の制限を使う。
- 開発者向け対策
  - コマンド解析でプロセス代入 >(...) を含む全パターンを検出し、必ずサンドボックス化して実行する。
  - マルチパートコマンドの承認UIは、チェーン内の全サブコマンドを明示して個別承認を求める。
  - CLI実行権限は最小権限（非root、限定ファイルアクセス、Network egress制限）に制限する。
- 組織的対策（組織・日本市場向けの注意）
  - 開発PCが社内ネットワークに接続している場合、出口（egress）トラフィックを監視・制限する（プロキシ、WAF、DNSログ）。
  - 機密データを含むリポジトリやCI環境では、公開AIサービスを直接つなげない、あるいは隔離された環境（コンテナ／一時VM）で動かす。
  - セキュリティポリシーに「AIエージェント利用のガイドライン」を盛り込み、現場での誤設定を防ぐ。
- ユーザー向け簡単チェックリスト
  - Bobや類似ツールを使う前に「auto-approveが有効か？」を確認。
  - 不審なREADMEや外部リソースに対してはローカルでの実行を避ける。
  - 重要な開発環境では、AIのコマンド実行機能を無効化することも検討。

おわりに：AI支援ツールは強力ですが「便利さ」と「危険」を天秤にかけた設計・運用が不可欠です。日本の現場でもすぐに取り組める対策を優先して実施してください。
