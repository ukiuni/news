---
layout: post
title: "Expensively Quadratic: The LLM Agent Cost Curve - 高コストの二次曲線：LLMエージェントのコスト曲線"
date: 2026-02-16T07:38:52.951Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://blog.exe.dev/expensively-quadratic"
source_title: "Expensively Quadratic: the LLM Agent Cost Curve - exe.dev blog"
source_id: 47000034
excerpt: "会話履歴の読み取り費が呼び出し増で数万トークンから爆発的に膨らむ問題と、設計で抑える具体策を解説"
---

# Expensively Quadratic: The LLM Agent Cost Curve - 高コストの二次曲線：LLMエージェントのコスト曲線
チャット履歴があなたの請求を膨らませる—50,000トークンで“読み取り”費が支配する理由

## 要約
LLMを使う「エージェント」は会話履歴を毎回渡すため、キャッシュの読み取りコストが会話長と呼び出し回数に応じて急増する（見かけ上は二次的）。実運用では数万トークンで読み取りが総コストを支配する例が観測されています。

## この記事を読むべき理由
日本のスタートアップや開発チームがLLMベースの自動化・コード支援を導入する際、トークン課金構造を無視すると運用コストが急増します。特に大規模リポジトリや頻繁な往復が発生するワークフローでは要注意です。

## 詳細解説
- エージェントの基本ループ：ユーザー入力 → LLM呼び出し（会話履歴を含む）→ ツール実行 → 次の入力へ。毎回「履歴の読み取り（cache read）」「前回出力の書き込み（cache write）」が発生します。  
- 課金要素：入力トークン・出力トークン・キャッシュ書き込み・キャッシュ読み取りが別々に課金されるため、呼び出し回数が増えるほど歴史全体の読み取りが積み上がる。単純化すると読み取りコストは「トークン数 × 呼び出し回数」に依存しやすく、これが二次的に増える主因です。  
  $$\text{Cost}_{\text{reads}} \propto T \times N$$
  （$T$＝会話のトークン長、$N$＝LLM呼び出し回数）  
- 実データ例：ある会話では総コストの87%がキャッシュ読み取りで、半分を超えたのは約27,500トークン。exe.devの集計でも多数の会話で読み取りが急増する傾向が確認されています。  
- 料金モデルの影響：例としてAnthropic系の料金比率（入力 x、書き込み 1.25x、出力 5x、読み取り x/10、x=$5/百万トークン）を用いたシミュレーションでは、設定次第で2万トークン程度で読み取りが支配的になることも示されました。  
- トレードオフ：頻繁にフィードバックを入れて精度を上げるほど呼び出し増→コスト増。一方、呼び出しを減らすと「デッドレコニング（手戻り）」リスクで誤った方向に進む可能性あり。

## 実践ポイント
- 呼び出し回数を減らす設計を優先する：必要な対話をまとめて一度でやる、長いツール出力は分割せず一括で扱う。  
- サブエージェント／ツール側でLLMを呼ぶ：メインのコンテキスト外で反復処理させることでメイン履歴の読み取りを抑える。  
- 会話を切り替える勇気を持つ：新しいタスクは新しい会話（リポジトリ起点で再開始）にし、長時間続けるより再確立が安い場合がある。  
- キャッシュポリシーを見直す：どこまでキャッシュするか（全文か要約か）で読み取りコストが大きく変わる。要約キャッシュや差分だけ保存する工夫を。  
- メトリクスを計測する：トークン数・呼び出し回数・各コスト内訳を可視化し、閾値（例：20k～50kトークン）でアラートを立てる。  
- 日本の現場向け：大規模モノレポや多数のCI／レビュー自動化では特に効く対策。コスト試算をプロジェクト初期に入れることを推奨。

以上を踏まえ、LLMエージェントの設計は「性能」と「トークンコスト」のトレードオフの上に成り立ちます。まずは呼び出し回数とキャッシュ設計を見直して、運用負担を抑えることをおすすめします。
