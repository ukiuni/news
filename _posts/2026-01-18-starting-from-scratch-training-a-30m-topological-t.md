---
layout: post
title: "Starting from scratch: Training a 30M Topological Transformer - 30Mパラメータの「トポロジカル・トランスフォーマー」を一から訓練する"
date: 2026-01-18T12:38:53.244Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://www.tuned.org.uk/posts/013_the_topological_transformer_training_tauformer"
source_title: "013_the_topological_transformer_training_tauformer - AI Research Engineering"
source_id: 46666963
excerpt: "ラプラシアン由来の距離で注意を置換し、30Mで高速収束・KVキャッシュ半減を示したTauGPT"
---

# Starting from scratch: Training a 30M Topological Transformer - 30Mパラメータの「トポロジカル・トランスフォーマー」を一から訓練する
注目技術：Attention を「距離（スカラー）」で置き換える――Taumode でAttentionの位相構造を直接注入する新しい試み

## 要約
Tauformer（TauGPT）は、従来のドット積Attentionをトークンごとのラプラシアン由来スカラー（taumode／λ）に置き換え、スカラー空間の距離で注目先を選ぶことでドメイン構造を直接利用しようとするモデルです。30Mパラメータ級モデルの短時間実験で有望な収束挙動が確認されました。

## この記事を読むべき理由
- Attentionの「比較軸」を内在化する新提案は、ドメイン固有の関係性を明示的に扱えるため、日本の企業が保有する専用データ（製品カタログ、業務ドキュメント、日本語コーパスなど）を効率的に学習・推論する際に有用になり得ます。  
- KVキャッシュ削減や疎ラプラシアン利用など、計算資源やメモリの節約に直結する設計は国内のコスト重視の現場で魅力的です。

## 詳細解説
- 基本アイデア  
  Tauformerは各ヘッドのベクトルをスカラーに圧縮し、そのスカラー（taumode / λ）間の距離でAttentionのロジットを作ります。これにより「ドメイン記憶（domain memory）としてのグラフラプラシアン $L$」を参照し、ドメイン固有の関係を優先するようAttentionをバイアスします。

- Taumode（λ）の計算（実装上の要点）  
  各ベクトル $x$ に対してRayleigh商類似のエネルギーを計算し、定数で束縛して $[0,1)$ に写像します。式は次の通りです：
  $$E_{\text{raw}}(x)=\frac{x^\top L x}{x^\top x + \varepsilon}$$
  $$\lambda_\tau(x)=\frac{E_{\text{raw}}(x)}{E_{\text{raw}}(x)+\tau}$$
  そしてAttentionロジットはスカラー差の負値距離で与えられます：
  $$\text{att}_{ij} = -\frac{|\lambda^Q_i - \lambda^K_j|}{\text{temperature}}$$

- 実装上はQ/K/V投影、RoPE（回転位置埋め込み）、因果マスク、softmaxによる重み付けなど既存パイプラインを維持しつつ、ロジットの計算だけを置き換えます。

- メモリ／計算コストの利点  
  KVキャッシュに保存するのが従来の$(K,V)$ではなく$(\lambda_k, V)$になるため、キー側がスカラー化されおおよそレイヤー毎に約50%のキャッシュ削減（ヘッド次元等で変動）を見込みます。さらに、ラプラシアンを疎行列で事前構築すれば、$\lambda$算出コストは密行列積 $D^2$ に依存しない設計が可能です。

- 30Mモデルの訓練設定（要点）  
  - モデル: TauGPT 約30M（6層、6ヘッド、埋め込み384、seq_len 1024）  
  - Optimizer: AdamW, base LR = 5e-4, warmup 100 steps  
  - データ: JSONL を IterableDataset でストリーム、20バッチに1回を検証に回す（約5%）  
  - 実験では taumode は固定（次実験で適応的再較正へ）

- 結果ハイライト  
  - 初期100ステップで val loss ≈ 4.93 → 2000ステップで val loss ≈ 2.36（PPL 6.6） → 最良は step 4500 で val_loss ≈ 1.9146  
  - ただし後半はバリデーションが不安定で退行する局面もあり、固定taumodeの限界が示唆される  
  - トレーニングは短時間（数時間）で進み、60K tokens/s 程度の処理速度が報告

- taumode と損失の相関（解釈と注意点）  
  - 損失低下とともにλ（エネルギーの中央値）が下がる傾向が観察される。これは学習によりK表現が「ラプラシアン上で滑らか」になったためか、あるいは表現が収縮して識別性を失う（collapse）ためかの両面の可能性がある。  
  - 再較正手順が中途で挿入されると、単なる観測値の変化がハイパーパラメータ変更の影響に変わり得るため、中央値だけでなく分布の広がり（p05/p95）も監視する必要があります。

- 理論的接続（epiplexity と arrowspace）  
  著者は、この種の「スカラー圧縮」が有限計算資源の学習器に追加の可学習構造を与えるという観点（epiplexity）を示唆。arrowspace での事前構築ラプラシアンと組み合わせることで、情報を下流計算が扱いやすい形に再因子化できる可能性があります。

## 実践ポイント
- 小さめのGPT実装で試す：まずはミニモデル（数万〜数百万トークン）で $L$ を疎に構築し、$\lambda$計算とKVキャッシュの差を確認する。  
- Taumode監視：中央値（p50）だけでなく p05/p95 をログ化し、収束と「スプレッド縮小（collapse）」を区別する。  
- キャッシュ測定：KVキャッシュのメモリ削減と推論速度への実際の影響をベンチマークする（特に日本企業の業務データを使ったドメインモデルで効果を確認）。  
- 再較正戦略を試す：固定taumode → 定期再較正（バッチ中央値） → 勾配依存のゲーティング再較正、など複数戦略を比較し安定性を評価する。  
- 日本向け応用例：製品説明文やFAQのようなドメイン固有コーパスを使い、ラプラシアンを「用語間のグラフ」で設計すると自然言語の意味的な近さを取り込みやすい。  
- スケーリング計画：30Mでの有望性を受け、100M級での再評価を行う。計算コスト削減の恩恵がより顕著になるかを確認する。

短くまとめると、Tauformerは「Attentionの比較軸をドメインラプラシアン由来のスカラーに置き換える」ことで、ドメイン構造の直接活用とKVキャッシュ削減を同時に狙うアプローチです。日本の現場でのドメイン特化モデルやコスト最適化に直結する可能性が高く、まずは小スケールでのプロトタイプ検証を強く推奨します。
