---
layout: post
title: "Keeping 20,000 GPUs healthy - 2万台のGPUを健全に保つ"
date: 2026-01-12T01:10:01.537Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://modal.com/blog/gpu-health"
source_title: "Keeping 20,000 GPUs healthy"
source_id: 1421045187
excerpt: "2万台GPU運用の具体策：クラウド選定・イメージ・健診運用ノウハウ"
image: "https://modal.com/docs/social-image.png?title=Keeping+20%2C000+GPUs+healthy&amp;socialType=blog"
---

# Keeping 20,000 GPUs healthy - 2万台のGPUを健全に保つ
2万台のGPUをスケール運用する現場から学ぶ、クラウド選定・イメージ設計・健診・可観測性の実践ルール

## 要約
Modalが20,000台超の同時GPUを運用する中で築いた、クラウド選定、マシンイメージ運用、起動時チェック、受動／能動の健診フロー、観測とサポートまでの実務ノウハウを紹介する。

## この記事を読むべき理由
GPUは高性能だが信頼性問題が多く、日本のクラウド利用者や研究開発チームも同じ課題に直面する可能性が高い。スケール運用で得られた具体的手法は、クラウドコスト・信頼性の両立に直結する実践的な知見になる。

## 詳細解説
- 全体像  
  Modalは複数ハイパースケール事業者（記事は匿名でA/B/C/D）からオンデマンドで数百万台規模のインスタンスを調達し、20k超GPUを同時運用。スケールが大きいほど「ほぼすべてのGPU故障パターン」を見るようになる。

- インスタンスタイプ選定の差  
  各クラウドは同じGPUでも信頼性・性能・起動APIの確度が異なる。例：Cloud Aは起動APIが安定しブート成功率99.6%で起動も速いが、H100のテキスト→画像性能は他に比べ50%劣る。Cloud CはH100の冷却が不十分で90°C超になりやすく、70°C台半ばからFLOP/sが低下する。Cloud Dは価格性能比が優れるが、特定リージョンでA10のハードウェアスローダウンやECCエラーが多発した。（プロバイダ固有の調査と内部ペナルティ課金で運用コストを補正）

- SXM vs PCIe の性能差（サンプル）  
  - matmul時間: SXM 1.62s vs PCIe 2.72s（約67%）  
  - FLOPS: 678 TF/s vs 405 TF/s（-40%）  
  - h2d pageable: 7.7 GiB/s vs 21.0 GiB/s（差異はワークロード依存）  
  → 同一世代GPUでもフォームファクタ／接続で大きく変わるため要ベンチ。

- マシンイメージ（AMI等）運用  
  同一カーネル/ドライバ（例: NVIDIA driver 580.95.05 など）を全クラウドで揃えることが信頼性向上に直結。ModalはイメージのCI化と段階的ロールアウト、ビルド完了時にDCGMやカスタムGPUテストを実行してから本番へ昇格させる。

- 起動時のトレードオフ  
  深いハードウェア診断（dcgmi diag --run 4 等）は時間がかかり、オートスケールの遅延やフォールオーバー低下を招く。Modalは軽い起動チェック（systemctl/nvidia-smi/ランダムGPUの基本RW）を行い、重大な問題はその後のパッシブ/アクティブ健診で検出する方針。L4カードでCUDA初期化が0.1%失敗する例があり、アプリ側でcuInitリトライを推奨。

- 生涯（ライフタイム）健診  
  - パッシブ（非占有・読み取り）: dmesg、dcgmi health でECCエラー、温度異常（>88°C）、HW_SLOWDOWNなどを監視。  
  - アクティブ（占有・負荷）: 週次でDCGM diag level2、GPUBurn、NCCLローカルall-reduceなどを実行。失敗が出れば隔離・再イメージ・破棄を行う。将来的にはInfiniBandやib_write_bw/latencyのネットワークテストも追加予定。

- 対処方針と観測性  
  GPU単体のリセットで回復するケースもあるが確実性が低いため、Modalはホスト丸ごと不健康扱いにしてドレイン→廃棄／再インストールする運用を採用。ダッシュボードではコンテナごとにメモリ使用率・利用率・温度・消費電力を提供し、異常イベントはログに "gpu-health" として流す。Xidエラー辞書などナレッジを整備。

- サポート体制  
  企業顧客向けは専用Slack＋チケット統合で迅速対応。一般向けはコミュニティとクレジット補償でカバー。これらで四つの9（99.99%）に近い稼働率を目指す。

## 実践ポイント
- インスタンス選定は「スペックだけでなく信頼性と起動性」を評価する。ベンチでSXM vs PCIe等の違いを確認する。
- マシンイメージはCI化してカーネル／ドライバの一貫性を保ち、イメージビルド時にDCGM等で実機検証を行う。
- 起動時チェックは「短く早く」：軽めのヘルスチェックで速く起動させ、継続的なパッシブ／週次アクティブチェックでカバーする設計にする。
- アプリ側での防御（例: cuInit のリトライ）や、単一GPU不具合の検出を容易にするログ・メトリクス設計を行う。
- 重大なGPU異常ではホスト全体を隔離して再イメージ/交換する運用ルールを用意する（まさに「壊れても迅速交換」できる供給・契約設計が鍵）。

以上はModalの実運用から得た知見で、国内のクラウド利用者や研究チームでも適用できる実戦的なチェックリストになる。
