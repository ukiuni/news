---
layout: post
title: "Grok turns off image generator for most users after outcry over sexualised AI imagery - Grok、性的に描写されたAI画像をめぐる抗議受け多数ユーザー向け画像生成を停止"
date: 2026-01-09T10:18:49.725Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://www.theguardian.com/technology/2026/jan/09/grok-image-generator-outcry-sexualised-ai-imagery"
source_title: "Grok turns off image generator for most users after outcry over sexualised AI imagery | Grok AI | The Guardian"
source_id: 468561242
excerpt: "Grokが女性写真を無断で性的加工、画像生成を有料化し大規模停止へ"
image: "https://i.guim.co.uk/img/media/5602890ef620af5b8c02784d85062ad26bfaa4a7/308_0_3083_2467/master/3083.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=4925f43d7d0ea5a6b634e63b07f06559"
---

# Grok turns off image generator for most users after outcry over sexualised AI imagery - Grok、性的に描写されたAI画像をめぐる抗議受け多数ユーザー向け画像生成を停止
魅力的なタイトル: 「AIで“裸にされた”女性たち——Grokの画像機能停止が示す、私たちの次の選択肢」

## 要約
Elon MuskのAIツール「Grok」が、女性の画像を無断で性描写に加工する事例が相次いだため、多くのユーザーに対して画像生成・編集機能を停止し、有料会員限定に切り替えた。英国では規制当局や政府が強く反応している。

## この記事を読むべき理由
AI画像生成が一般化する中で、技術的な制御と規制のギャップが顕在化しています。日本の開発者やサービス運営者、一般ユーザーにとっても、プライバシー・法規制・運用設計の教訓が得られる事件です。

## 詳細解説
- 何が起きたか  
  Grokの「Imagine」系機能が更新された直後、被写体の同意なしに女性の写真を“裸にする”“性的な場面に置く”といった加工が大量に作成され、SNS上で拡散。NG行為として非同意の性的画像（non-consensual sexual imagery）や暴力的・猟奇的な合成映像も確認された。

- 事業者側の対応  
  x（旧Twitter）とGrokは、画像生成・編集を有料会員のみ許可し、利用者の本人情報とクレジットカード情報を保持して、濫用時に追跡可能にすると発表。英国ではOnline Safety Actに基づきOfcomや政府がサイトブロッキングや巨額の罰金（最大でグローバル売上の10%）を示唆し、強い圧力がかかった。

- 技術的問題点  
  1) モデルは指示（プロンプト）次第で誰でも容易に特定個人を性的に描写できる。  
  2) フォトリアルな動画生成が可能になり、検出と削除が困難。  
  3) モデルのトレーニングデータに含まれる既存の画像やバイアスが、危険な出力を生む。  
  4) コンテンツ検知には誤検出・過検出の課題があり、完全自動化は未成熟。

- モデル運用で考えるべき防御策（技術観点）  
  - 出力の「アクセス制限」：有料化・本人確認・KYCの導入。  
  - 技術的抑止：出力に不可逆な透かし（デジタルウォーターマーク）を付与し、生成ログを保存。  
  - モニタリング：疑わしいプロンプトや高リスクメディアを検出するフィルタ・スコアリング。  
  - 人間とAIのハイブリッド審査：自動検出でフラグ→人的レビューで最終判断。  
  - データ管理：トレーニングデータの精査、同意のない素材の排除。  
  - 検出ツールの活用：ディープフェイク検出器やメタデータ解析を導入。

## 日本市場との関連
- 法的観点：日本でも肖像権、名誉毀損、児童に関する犯罪規制や個人情報保護法の観点から問題化し得ます。プラットフォーム事業者は国内法令と社会的コンセンサスを踏まえた対応が求められます。  
- 事業運営：国内SNS事業者やAIベンダーは、海外の対応事例を踏まえたコンプライアンス、利用規約の整備、通報窓口の強化が重要です。特に日本では被害者の告発が心理的に難しいことも多く、事前防止策の重要性が高いです。  
- 市場機会：透明性や安全性を売りにするAIサービス、生成物の出所・改変履歴を残すプロダクトには信頼獲得の余地があります。

## 実践ポイント
- 開発者向け  
  1) 高リスク生成をブロックするプロンプト検出ルールを早急に実装する。  
  2) 生成ログ（プロンプト、出力、ユーザーID）を保存し、濫用時に追跡可能にする。  
  3) 生成物に透かしやプロビナンス（出所情報）を埋め込む設計を検討する。  
  4) トレーニングデータの同意・ライセンスチェックを必須化する。

- 事業者/運営者向け  
  1) 利用規約と懲罰ルールを明示し、違反時の即時措置を運用化する。  
  2) ユーザー通報フローと人的レビュー体制を整備する。  
  3) 法的リスク（国内外）を評価し、必要なら利用制限やKYCを導入する。

- 一般ユーザー向け  
  1) 画像を公開する際は高解像度で顔が特定されるものを避ける。  
  2) 非同意の合成を見つけたら即通報する。  
  3) 利用するサービスのポリシーと出力の透かし有無を確認する。

短く言えば、この事件は「技術が可能にしたこと」と「社会が許容できること」のズレを露呈しました。日本の技術者や運営者は、早急な技術的・運用的対策と透明性で信頼を築く必要があります。
