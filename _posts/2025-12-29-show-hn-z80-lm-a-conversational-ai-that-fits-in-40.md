---
layout: post
title: Show HN: Z80-μLM, a 'Conversational AI' That Fits in 40KB - Show HN: Z80-μLM、40KBに収まる「対話型AI」
date: 2025-12-29T06:52:42.627Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://github.com/HarryR/z80ai"
source_title: "GitHub - HarryR/z80ai: Z80-μLM is a 2-bit quantized language model small enough to run on an 8-bit Z80 processor. Train conversational models in Python, export them as CP/M .COM binaries, and chat with your vintage computer."
source_id: 46417815
excerpt: "4MHzのZ80で40KB実行、会話する超小型AIの仕組み紹介"
---

# Show HN: Z80-μLM, a 'Conversational AI' That Fits in 40KB - Show HN: Z80-μLM、40KBに収まる「対話型AI」

## 要約
わずか約40KBのCP/M .COMバイナリで、1970年代のZ80（4MHz、64KB RAM）上で会話風の応答を生成する2ビット量子化言語モデル。レトロマシンでチャットを楽しめるだけでなく、極限まで制約された環境でのML設計の教科書的実装でもある。

## この記事を読むべき理由
日本の組込み・レトロコンピューティング愛好家、IoTやTinyMLの実務者、教育で「最小限の資源で動く学習モデル」を示したい人にとって、設計思想と実装テクニック（量子化、ハッシュ化入力、整数専用推論）は即役立つ知見となる。

## 詳細解説
- 基本コンセプト  
  モデルは「会話する風」な応答を文字単位で自動生成する非常に小さなニューラルネットで、サイズ削減のために2ビット量子化（各重みが {-2, -1, 0, +1} のいずれか）を採用。学習はPython上で行い、CP/M用の.COM実行ファイルとしてエクスポートしてZ80上で推論する。

- 入力表現：トライグラム・ハッシュ  
  入力テキストはトライグラム（3文字の並び）をハッシュして128バケットの“タグクラウド”に落とし込み、バケットごとにスコアを集計する方式。語順に頑健でタイポ耐性があり、短文の意図分類に向く一方、長文や順序依存の命令文には弱い。

- ネットワーク構成  
  入力：128（クエリ）＋128（文脈）バケツ  
  隠れ層：可変（例：256→192→128）  
  出力：文字セットごとに1ユニット（文字ごとの確率を出して1文字ずつ生成）  
  活性化：隠れ層はReLU、全て整数（16-bit）で計算

- 量子化と整数推論  
  2ビット重みはバイトに4つ詰めて保持。2ビット値vを重みwへは
  $$ w = v - 2 $$
  によりマップ（0→-2,…,3→+1）。推論は16ビット符号整数の乗算加算（MAC）で行い、1層ごとに符号を保った右シフトでスケールダウンしてオーバーフローを抑える：
  $$ \mathrm{ACC} \leftarrow \sum_i w_i x_i $$
  $$ \mathrm{ACC} \leftarrow \mathrm{arithmetic\_shift\_right}(\mathrm{ACC}, 2) $$
  こうした整数専用パスがあるため、浮動小数点がない古典的CPUでも安定して動く。

- Z80向け最適化（コアループ）  
  Z80アセンブリで重みをアンパック→乗算（+1/−1/−2 は加減算で代替）→加算という最小命令列を回し、1文字生成あたり約10万回の内ループを回す。重み=0はスキップし、負荷を減らす工夫がある。

- 実例と制約  
  付属サンプルに「tinychat」（挨拶や人格付けの回答）と「guess」（20質問のYES/NO/MAYBE型）あり。短い応答中心で多層文脈追跡や自然言語理解を期待するのは不向きだが、制約下での“表現”として十分に魅力ある出力を生む。

- 学習上の注意点（QAT）  
  2ビットという極端な量子化下では通常の訓練では収束や表現力が損なわれるため、量子化認識トレーニング（QAT）が必須。オーバーフロー回避のためアクティベーションや学習率の調整、クラスバランスの改善が重要。LLMを用いたデータ拡張（例：会話例生成）も組み込まれている。

## 実践ポイント
- CP/Mエミュレータでまず動かす：エミュレータ（例えばCP/M互換環境）に.COMを置けばすぐに試せる。実機（PC-8801／FM-7等）と接続するレトロ工作が好きな人はイベント向けデモに最適。
- 学習データ作成：短いQ&AやYES/NOゲームが向く。LLMでサンプルを生成し、クラス（応答候補）の分布を揃えると良い。
- QATを重視：通常のフル精度で学習→量子化ではなく、量子化を考慮したトレーニングフローを採ること（勾配クランプや学習率低下など）。
- 言語・文字セットを絞る：文字セット（charset）を限定すると出力質が上がり、モデルサイズも下がる。日本語は1文字当たりのバイト幅が問題になるため、用途に応じてローマ字や限定語彙を検討。
- 応用アイデア：極小チャットボット、ハードウェア遊び、教育教材（「なぜ量子化が必要か」を体感させる）、IoT機器の超軽量応答モジュール。
- 限界を受け入れる：多ターン文脈理解や自由生成は期待しない。代わりに「短い問いかけに対する個性のある返答」や「遊び／デモ用途」を狙うと満足度が高い。

最後に一言：最新の大規模モデルとは別ベクトルの“ミニマルで楽しい”AI実装。制約を逆手に取る設計は、学びも驚きも与えてくれます。ライセンスはMIT/Apache系で扱いやすく、試作や教育プロジェクトとして始める敷居は低いです。
