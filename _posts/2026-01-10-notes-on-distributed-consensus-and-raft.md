---
layout: post
title: "Notes on Distributed Consensus and Raft - 分散合意とRaftのメモ"
date: 2026-01-10T17:48:12.203Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://cefboud.com/posts/distributed-consensus-raft-kafka/"
source_title: "Notes on Distributed Consensus and Raft | Moncef Abboud"
source_id: 467468923
excerpt: "Raftでログと過半数合意を使い、KafkaやKubernetesの高可用性を支える仕組みを解説"
image: "https://cefboud.com/assets/img/favicons/og.png"
---

# Notes on Distributed Consensus and Raft - 分散合意とRaftのメモ
止まらないサービスの秘密：Raftで実現する「全員合意」の仕組み

## 要約
分散システム（例：KafkaやKubernetes）が複数台で同じ状態を保てる理由は「分散合意」にあり、Raftはその合意をわかりやすく実現するアルゴリズムです。ログの順序保証、リーダー選出、過半数（クォーラム）での確定が鍵になります。

## この記事を読むべき理由
日本のサービス運用でも、LINEや楽天、メルカリ級のトラフィックや可用性を考えると「障害時にダウンさせない仕組み」は必須です。Raftの基本を押さえれば、Kafkaの設定やetcdを使ったKubernetesクラスタ運用で落とし穴を避けられます。

## 詳細解説
なぜ複数台が必要か
- スケール：垂直拡張には限界があり、水平分散で負荷を捌く必要がある。  
- 信頼性：ハードウェアは故障するため冗長化が必須。  
- レイテンシ／性能：地域別配置や負荷分散で応答性を保つ。

共有状態（メタデータ）をどう保つか
- Kafkaでは、どのパーティションがどのブローカーにあるか、どれがリーダーか等のメタデータを全ノードで一致させる必要がある。  
- 状態そのものを頻繁に丸ごと送るのではなく、「状態変化のイベント（ログ）」を順序どおり共有することで最終的に同じ状態に到達する。

Raftの主要要素（技術ポイント）
- ログレプリケーション：クライアントから来た変更はまずリーダーに送られ、リーダーが追随ノードに複製して過半数の確認を得てコミットする。  
- リーダーとフォロワー：クラスタには1つのリーダーが存在し、他はフォロワー。リーダーが中心となってログを流す。  
- クォーラム（過半数）：コミットは過半数の承認が必要。これにより一貫性が保証される。  
- 選挙とタイムアウト：リーダーが音信不通になるとフォロワーは選挙タイムアウト後に候補者となり投票を呼びかける。ランダム化されたタイムアウトで分裂投票を避ける。  
- クォーラム喪失時：ノードが多数落ちるとコミットできない変更が発生する（ローカルには存在しても未確定）。復帰したノードはリーダーから差分を取り込み追いつく。

実装上の差分
- KafkaのKRaftはRaftの変種で、フォロワーが更新をプルする設計になっている点が異なるが、基本原理（ログで状態を再現する、選挙、クォーラム）は同じ。  
- Kubernetes自体はRaftを直接実装せず、etcdという分散K-Vストア（内部でRaftを使用）に依存している。

## 実践ポイント
- ノード数は奇数にする：過半数計算で無駄を減らす（例：3/5/7）。  
- レプリケーション係数を適切に設定：Kafkaのreplication.factorやetcdのクラスタサイズで可用性を確保。  
- 障害実験を定期的に行う：リーダー単体停止、ネットワーク分断、遅延を加えてフェイルオーバー挙動を確認する。  
- メトリクスを監視：リーダー切替頻度、未コミットのログ数、選挙の発生回数を監視してチューニングする。  
- 小さな実験環境で触る：raft.github.ioの可視化やローカルのetcd/KRaftモードで動作を体験する。  
- 設計トレードオフを理解：可用性・一貫性・性能のどれを優先するかで設定と構成が変わる（CAPの観点）。

以上を理解すれば、KafkaやKubernetes周りの「なぜ動くのか／何が壊れるとまずいのか」が見えてきます。運用チームやアプリ開発者がRaftの基本を押さえておくだけで、障害対応と設計判断の精度が格段に上がります。
