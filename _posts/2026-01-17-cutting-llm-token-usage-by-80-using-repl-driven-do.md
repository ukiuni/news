---
layout: post
title: "Cutting LLM token Usage by ~80% using REPL driven document analysis - REPL駆動の文書解析でLLMのトークン使用量を約80%削減"
date: 2026-01-17T01:23:41.093Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://yogthos.net/posts/2026-01-16-lattice-mcp.html"
source_title: "(iterate think thoughts): Stop Round-Tripping Your Codebase: How to Cut LLM Token Usage by 80% Using Recursive Document Analysis"
source_id: 766504277
excerpt: "マトリョーシカでLLMの重複送信を防ぎ、トークン約80%削減"
---

# Cutting LLM token Usage by ~80% using REPL driven document analysis - REPL駆動の文書解析でLLMのトークン使用量を約80%削減
LLMに同じコードを何度も送り返させない──トークンを劇的に節約する「Matryoshka」流ドキュメント解析

## 要約
REPL風のドキュメント解析ツール「Matryoshka」は、ファイル内容を丸ごと毎回モデルに送らず、サーバ側で結果を保持することでトークン使用量を約80%削減する。大きなリポジトリを対話的に調べる際のコストと品質低下（context rot）を同時に改善する手法だ。

## この記事を読むべき理由
日本でも大規模なOSSや企業コードベースをLLMで解析する場面が増えている。API呼び出しのコストや応答品質が問題になる中、トークン節約と効率的な探索手法を実務で使える形で理解しておく価値が高い。

## 詳細解説
- 問題点（なぜ従来アプローチはまずいか）
  - 中〜大規模プロジェクトでは全ファイルを一度にモデルに渡すと数万〜十万トークンになる。対話で何度も同じ文書を送り返すとコストが爆発する。
  - 長い入力はモデルの出力精度を落とす「context rot（文脈劣化）」を招き、重要な情報の継ぎ合わせが難しくなる。

- 先行研究のエッセンス
  - Recursive Language Models（RLM）系の発想：文書を外部状態として扱い、必要な断片だけを問い合わせる。
  - Barlimanの発想：例示（入出力例）から必要なパーサ/関数を合成することで、手作業の正規表現や詳細設計を減らす。

- Matryoshkaの核となる仕組み
  1. Nucleus：S式風の宣言型クエリ言語で「何を得たいか」を記述。操作の手順ではなく目的を伝える。
  2. ポインタベースの状態管理：検索結果はサーバ側に保存され、LLMには「Found 150 results」のような要約と「RESULTS」という参照（ポインタ）だけが返る。以降の計算はその参照を操作することで進むため、同じテキストを何度も送らない。
  3. 例からの合成：具体例を与えると、そのパターンに従う抽出関数をサーバ側で合成して実行できる（通貨文字列→数値など）。

- 実装と運用の流れ（概要）
  - ドキュメントをロード：サーバにパースして保存（コンテキストにはメタのみ）。
  - インクリメンタルにクエリ：grep, count, filter, map 等で結果を絞り込み。各操作はサーバ側で完結し要約だけが返る。
  - セッションは自動期限（例：10分）でメモリ解放。複数操作をチェーンして解析を進める。

- エコシステム連携
  - Model Context Protocol（MCP）でLLMエージェント（例：Claude）とツールとして連携。エージェントはツールマニフェストを読み、必要に応じてload/query/helpを呼ぶ形で発見・学習する。

- 実例（anki‑connect解析）
  - 対象コードベース：約7,700行・17ファイル
  - 比較結果：全読み込みだと約95kトークン、Matryoshkaだけなら約6.5kトークン（ただし小さなファイルの詳細は欠ける）、ハイブリッド運用で約17kトークン（フルカバレッジ）→ 約82%の節約を達成。

## 実践ポイント
- ハイブリッド戦略を採る
  - 小さなファイル（数百行未満）は直接読み込んで必要な設定やデコレータを把握する。
  - 大きなファイルやREADMEのような長文はMatryoshkaでクエリして必要箇所だけ抽出する。

- キャッシュとポインタを活用する
  - 一度実行したgrepや集計はサーバ側に残して再利用する設計にすることで、同じブロックを繰り返し送らない。

- 例示で抽出処理を自動化する
  - 正規表現を書かず、代表的な入出力例を与えて抽出関数を合成させるワークフローを試すと手戻りが減る。

- ツール発見を簡単に
  - LLMと統合する場合はMCPや相当のツールマニフェストを用意し、helpコマンドでクエリ言語を学習させると汎用性が上がる。

- 運用上の注意
  - セッションの有効期限やメモリ消費を設計に組み込み、重要な中間結果は永続化するか再作成ルールを明確にする。
  - 完全なカバレッジが必要な場合は、Matryoshka単体ではなく「小ファイル直接読み込み + Matryoshka」のハイブリッドが現実的。

この考え方は、日本の開発現場でもコスト管理と解析精度を両立させる実践的なアプローチになる。まずは既存の解析フローで「大きなファイルだけ外部クエリに切り替える」ことから試してみると効果が見えやすい。
