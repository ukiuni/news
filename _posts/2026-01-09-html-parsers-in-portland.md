---
layout: post
title: "HTML parsers in Portland - ポートランドのHTMLパーサ"
date: 2026-01-09T09:12:27.308Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://felix.dognebula.com/art/html-parsers-in-portland.html"
source_title: "HTML parsers in Portland"
source_id: 953106269
excerpt: "AI移植のHTMLパーサが微妙に挙動や性能を変え、脆弱性と運用リスクを生む事例を検証"
---

# HTML parsers in Portland - ポートランドのHTMLパーサ
魅惑の「AI翻訳」で生まれた“似て非なる”HTMLパーサ、その危うさを覗く

## 要約
AIエージェントを使った「vibe-translation（ざっくり移植）」で生まれた複数のHTMLトークナイザは、テストを通る一方で実装の細部がばらばらに変質している。見た目は同じでも挙動・性能・保守性に微妙なズレが生じ、潜在的な脆弱性や運用リスクを残す可能性がある。

## この記事を読むべき理由
日本でもLLMを開発ワークフローに組み込む企業が増えています。外部コードや生成コードをそのまま受け入れる前に、こうした「テストを通すが意味が違う」問題を知っておくことは、プロダクトの安全性と保守性を守る上で必須です。

## 詳細解説
- 背景  
  元の記事は、Rustの手書きライブラリ html5ever を起点に、Python（JustHTML）→JavaScript→OCaml→Swift といった具合にAIが自動で“移植”した一連のHTMLトークナイザを追跡した調査です。各バージョンは見た目やAPIが似ているものの、一部の重要な振る舞いが異なっていました。

- トークナイザの核心（Data state）  
  HTMLトークナイザの「Data」状態は、テキスト部で次の文字を消費して処理を分岐します。仕様上の主要分岐は大きく次の通りです：  
  1. '&' → return state を保存して Character Reference 状態へ  
  2. '<' → Tag Open へ遷移  
  3. '\0'（NULL）→ エラー扱いして文字を出力  
  4. EOF → 終了トークンを出力  
  5. それ以外 → その文字を文字トークンとして出力

- 最適化と差分  
  hand-written の html5ever は「pop_except_from」で非特別文字を高速スキャンし、改行処理や行番号管理など最適化が入っています。一方、AIが生成した実装群は次のような違いを示しました：  
  - 一部実装で '&' の処理が別箇所へ移動・欠落している  
  - NULL文字の扱いが U+FFFD に置換されたり、エラー処理の有無が異なる  
  - 高速化の手法（範囲検索 vs 逐次処理）がバラバラで性能特性が変わる  
  - 改行正規化や reconsume ロジックが雑に混ぜ込まれ、可読性が低下  
  - ある訳では単純化されすぎてテストだけを通す最小実装になっている

- なぜ起きるか（“hallucination” の出所）  
  コーディングエージェントは「与えられたコンテキスト＋学習済みデータ」に基づきコードを生成します。結果として、元実装を逐語的に移すのではなく、トレーニングデータに含まれる別実装を“思い出して”再構築してしまうことがあります。形式的なテストは通るが、実際の振る舞い（境界ケース、性能、エラー報告の細部）が異なるケースが生まれます。

- 実例的懸念  
  テストスイート（html5lib-tests）を通したとしても、テストコード自体やテストランナーがvibeされていると検証の信頼度は下がります。記事はさらに、コメント整形の小さなバグ（"----" が "- -- -" になる）など、見落としやすい実装ミスが全派生に引き継がれている点を指摘しています。こうした些細な差異がセキュリティやログ・エラー処理で問題を招く可能性があります。

## 実践ポイント
- LLM生成コードは「草案」と考える  
  そのままマージせず、人間による機能的レビューと動作上の差分チェックを必須にする。

- 差分テスト（differential testing）を導入する  
  元実装（あるいは複数実装）と生成実装を同じ入力で比較し、出力やパフォーマンスに差がないかを確かめる。

- 境界ケースとファジングを追加  
  NULL文字、文字参照、改行のバリエーション、極端に長い入力などを使ったファジングで隠れた挙動を発見する。

- テストカバレッジを拡張する  
  仕様に明確に現れるケース以外にも、実運用でよく起きるパターン（エンコーディングの混在、部分破損データ等）をテストに加える。

- 可視化・ベンチマークをCIに組み込む  
  処理速度・メモリ消費・エラー頻度を継続監視し、あるコミットで急変が起きたら警告を出す。

- プロジェクト方針として「生成元のトレーサビリティ」を確保する  
  どのプロンプト／どのモデルで生成したか、生成後の人間レビュー記録を残す。重要ライブラリは「手作業での再実装」か「信頼できる実装のみ」を許可する。

- 型や静的解析で補強する  
  動的言語に生成する場合でも、型注釈・静的解析ルールを設けて不整合を検出しやすくする。

まとめ：LLMは強力な補助ツールだが「テストを通る＝正しい」ではない。特にパーサやプロトコル実装のように境界条件が多い領域では、生成コードの意味的同等性を人間が確認し、差分テストやファジングで隠れた挙動を洗い出す運用を必須にすべきだ。

（参照元: "HTML parsers in Portland"）
