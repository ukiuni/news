---
layout: post
title: "Trinity large: An open 400B sparse MoE model - Trinity large: オープンな400BスパースMoEモデル"
date: 2026-01-28T22:20:15.565Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://www.arcee.ai/blog/trinity-large"
source_title: "Arcee AI | Trinity Large: An Open 400B Sparse MoE Model"
source_id: 46789561
excerpt: "400BスパースMoE版Trinity Largeが2–3倍高速で512k長文対応"
---

# Trinity large: An open 400B sparse MoE model - Trinity large: オープンな400BスパースMoEモデル
400Bの“軽量化”で勝負する次世代モデル — Trinity Largeがもたらす「高速・低コスト・長文対応」の衝撃

## 要約
Arcee AIのTrinity Largeは、$400\mathrm{B}$パラメータのスパースMoE（Mixture-of-Experts）モデルで、$256$エキスパート中$k=4$を活性化する高いスパース性と効率化で「学習・推論ともに2–3倍高速化」を狙う。Preview／Base／TrueBaseの3種のチェックポイントを公開中。

## この記事を読むべき理由
日本のプロダクト開発者や研究者にとって、「長文コンテキスト（最大$512\mathrm{k}$対応）」「多言語（14言語をターゲット）」「コードや数式処理に強い」など実用性の高い特徴がある。無料プレビューで手を動かしやすく、運用コストを抑えた大規模モデル導入の参考になる。

## 詳細解説
- アーキテクチャとスパース性  
  Trinity Largeは$400\mathrm{B}$規模のスパースMoE。$256$エキスパートから$k=4$を選ぶルーティングで、トークンあたり有効パラメータは約$13\mathrm{B}$、ルーティング比率は約$1.56\%$。高いスパース比を維持するために密層を増やし（3→6層）、ルーティングの安定性を確保している。

- 学習・効率工夫  
  学習は$2048$台のNVIDIA B300で実行し、合計$17\mathrm{T}$トークンの事前学習（段階的に$10\mathrm{T}+4\mathrm{T}+3\mathrm{T}$）を33日で完了。HSDP＋expert parallelism=8で$2048$のデータ並列ランクを使い、Muonオプティマイザとバッチ増加によるスループット改善を実施。結果として同クラスの密なモデルより$2$–$3\times$高速な学習／推論を実現したと報告。

- 安定化手法  
  モデル内部では「モーメンタム付きのエキスパート負荷バランス（tanhでクリップ）」「シーケンス単位のバランス損失」「z-lossによるロジット発散抑制」など複数の工夫でルーティングと学習安定性を維持。

- データと多言語・合成データ  
  DatologyAIによるキュレーションで、プログラミング・STEM・推論・多言語を含む混合データを用意。合成データは8兆トークン規模で生成・活用し、特に非英語データの比重を高めている。

- チェックポイントの違い  
  - Preview：軽いポストトレーニング済みの「指示型（instruct）」でチャットや創作に向く（OpenRouterでプレビュー提供）。  
  - Base：フル$17\mathrm{T}$レシピ後の最良事前学習チェックポイント（強力な基礎モデル）。  
  - TrueBase：$10\mathrm{T}$時点の事前学習のみ（指示データ・LRアニールなし）。研究用ベースラインとして価値が高い。

- 実運用面  
  コンテキストは最大$512\mathrm{k}$サポート（プレビューは$128\mathrm{k}$で8-bit量子化動作）。学習コストは約\$20Mと発表されており、フロンティアクラスを比較的低コストで達成した点が注目。

## 実践ポイント
- まずはPreviewをOpenRouterで触る（無料期間あり）——チャットや創作、長文要約の試験運用に最適。  
- 長文ドキュメント検索やログ解析、契約書レビューなど$128\mathrm{k}$〜$512\mathrm{k}$活用が有効。  
- TrueBaseは「事前学習のみ」の挙動観察・研究用に最適。独自の微調整やRLHF前の分析に使える。  
- コード系エージェントにはまだ粗さがあるため、本番投入前に十分な検証を。  
- 日本語／業界固有データで追加微調整すれば実用性が一段と高まる（多言語訓練の恩恵はあるが、ドメイン適合が鍵）。

以上。Trinity Largeは「大きくて速い」「長文・多言語に強い」「公開チェックポイントで検証しやすい」点で、日本の開発現場や研究コミュニティにとって実務的に面白い選択肢です。
