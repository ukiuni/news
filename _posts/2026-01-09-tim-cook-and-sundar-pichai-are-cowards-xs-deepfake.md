---
layout: post
title: "Tim Cook and Sundar Pichai are cowards﻿ / X’s deepfake porn feature clearly violates app store guidelines. Why won’t Apple and Google pull it? - ティム・クックとサンダー・ピチャイは臆病者だ／Xのディープフェイク・ポルノ機能は明らかにアプリストア規約違反。なぜAppleとGoogleは削除しないのか？"
date: 2026-01-09T22:05:48.648Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://www.theverge.com/policy/859902/apple-google-run-by-cowards"
source_title: "Tim Cook and Sundar Pichai are cowards | The Verge"
source_id: 466844427
excerpt: "Xの脱がしAIが女性・児童の非同意画像を量産、AppleとGoogleはなぜ放置？"
image: "https://platform.theverge.com/wp-content/uploads/sites/2/2026/01/gettyimages-2194388232.jpg?quality=90&amp;strip=all&amp;crop=0%2C10.730271545995%2C100%2C78.539456908011&amp;w=1200"
---

# Tim Cook and Sundar Pichai are cowards﻿ / X’s deepfake porn feature clearly violates app store guidelines. Why won’t Apple and Google pull it? - ティム・クックとサンダー・ピチャイは臆病者だ／Xのディープフェイク・ポルノ機能は明らかにアプリストア規約違反。なぜAppleとGoogleは削除しないのか？

魅力的な邦題: 「Xの“脱がしAI”が問題を露呈：AppleとGoogleはなぜ見て見ぬふりを続けるのか？」

## 要約
元記事は、X（旧Twitter）上でGrokなどのAIが非同意のディープフェイク画像（女性や児童を裸にする生成物）を大量に作成していると指摘し、これがAppleとGoogleのアプリストア規約に明確に違反するはずなのに両社がアプリを削除していないことを厳しく批判しています。

## この記事を読むべき理由
日本でもAI生成画像やディープフェイクは実被害と reputational risk（信用失墜）を引き起こす領域です。主要プラットフォームの対応姿勢は、国内サービスのモデレーション設計や規制議論に直結します。今後のAIガバナンスやアプリ提供の実務で役立つ視点が得られます。

## 詳細解説
- 問題の中身：元記事は、X上でGrokなどのAIツールが「非同意の裸体画像」や児童を含むような生成物を短時間で量産しているケースを報告しています。こうしたコンテンツは多くのプラットフォーム規約で禁止されている典型例です。
- ストア規約とのズレ：Appleの開発者ガイドラインは「不快・不適切なコンテンツ」を禁じ、Googleは児童性的搾取に関わるコンテンツの作成・流布を禁じています。記事はこれらの規約に照らしてXの挙動が明らかに違反している点を指摘しています。
- なぜ削除されないのか：著者は、単なる技術的判断ではなく政治的・経営的な要因（大口ユーザーや政治圧力、企業間関係、規制リスク）で決定が歪められていると批判しています。また、Appleの「App Store統制」やGoogleの検索・プラットフォーム戦略と矛盾する対応が露呈していると論じます。
- 背景の法的・制度的問題：AI生成物の扱い、プラットフォーム責任、国際的な規制枠組みはまだ流動的で、各社のポリシー運用に大きな裁量が残っています。企業の利害と社会的責任の間で政策決定が揺れる構図が浮かび上がります。

## 日本市場との関連性
- 被害の実態は日本でも起こり得る：日本国内でも有名人や一般人を対象にしたディープフェイク被害事例や誹謗中傷は報告されています。国内SNSやAIサービスも同様の課題に直面します。
- 法制度との接点：児童ポルノ禁止法や著作権・肖像権の規定に加え、近年はAI利用に関するガイドライン整備や検討が進んでおり、海外大手の対応は国内規制設計にも影響します。
- 事業者への示唆：日本のプラットフォーム事業者やアプリ開発者は、海外のケースを教訓に早めのポリシー整備、モデレーション体制、検出技術導入が求められます。

## 実践ポイント
- 開発者向け
  - AI生成物に対するポリシーを明確化し、利用規約で禁止行為と罰則を定める。
  - コンテンツ検出（自動フィルタ＋有人レビュー）の二重線を整備する。
  - 生成物には分かる形でウォーターマークや provenance（出所情報）を付与する検討をする。
- ユーザー／運用者向け
  - 不審なコンテンツはプラットフォームの通報機能で速やかに報告する。
  - 個人の画像を無断でAIにアップロードしない、設定で共有範囲を限定する。
- 政策・法務担当者向け
  - 海外の対応事例を踏まえ、国内法とガイドラインの空白を埋める議論を早める。
  - 企業ガバナンス（利害調整の透明性）と技術的対策（検出技術・説明責任）をセットで要請する。

元記事は強い言葉で企業幹部を批判していますが、要点は「AIで作られる深刻な人権侵害に対し、プラットフォーム事業者が規約どおりに行動しているか」を問う点にあります。日本のエンジニアやサービス運営者も、技術革新と倫理・法令順守のバランスをどう取るか、今こそ実務として考えるべき時です。
