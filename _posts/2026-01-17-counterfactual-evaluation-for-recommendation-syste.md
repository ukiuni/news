---
layout: post
title: "Counterfactual evaluation for recommendation systems - レコメンドの反事実評価"
date: 2026-01-17T18:47:14.237Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://eugeneyan.com/writing/counterfactual-evaluation/"
source_title: "Counterfactual Evaluation for Recommendation Systems"
source_id: 46655524
excerpt: "A/Bテスト前に新ポリシーの効果をオフラインで高精度に推定する反事実評価入門"
---

# Counterfactual evaluation for recommendation systems - レコメンドの反事実評価
魅力的なタイトル: 「オフライン評価はもう古い？A/Bテスト前に“もしも”を推定する反事実評価入門」

## 要約
オフラインの履歴データだけでレコメンドを評価すると、本当にユーザー行動を変えられるかは分からない。A/Bテストを回さずに「もし新モデルを出していたらどうなったか」を推定するのが反事実（counterfactual）評価で、代表的手法にIPS、CIPS、SNIPSがあります。

## この記事を読むべき理由
日本のECやメディア、広告プロダクトでも「オフラインで良い指標なのに本番で伸びない」問題は頻発します。実際に本番で試す前に潜在的な改善効果を推定できれば、リスク低減と開発効率向上につながります。本稿は初心者にも分かるように技術的ポイントと実践での落とし穴を整理します。

## 詳細解説
なぜオフライン評価は問題か  
通常のオフライン評価は「観測（observational）」問題の枠組みです。商品情報からカテゴリを予測するような問題は観測問題で十分ですが、レコメンドは「どの推薦を出すか（介入）」によってユーザーのクリックや購入が変わるので、本質的には「介入（interventional）」問題です。過去ログに現れた行動は、過去に表示された推奨結果の影響下にあります。

反事実評価とは  
反事実評価は「もし別の推薦ポリシー π_e を使っていたら、ユーザー行動はどう変わったか」をオフラインで推定する手法群です。代表的なのが逆確率重み付け（Inverse Propensity Scoring, IPS）です。直感としては、ログ上の各観測を「新ポリシーがそのアクションをどれくらい出すか／旧ポリシーが出した確率」で重みを変えて平均する、というものです。

IPS の式（代表形）  
$$
\hat{V}_{IPS} = \frac{1}{n}\sum_{i=1}^n r_i \frac{\pi_e(a_i\mid x_i)}{\pi_0(a_i\mid x_i)}
$$
- $r_i$：観測報酬（クリック/購入など）  
- $a_i$：ログで実際に出された推奨（アクション）  
- $x_i$：文脈（ユーザーやページ）  
- $\pi_0$：既存（ログ取得時の）ポリシーの確率  
- $\pi_e$：評価したい新ポリシーの確率

重要な概念：確率（propensity）  
IPSには「各推奨がログでどのくらい出されたか（確率）」が必要です。これが無いと重み算ができません。公的データセットにはほとんど含まれないため、実運用では印象数（impression count）やスコアの正規化（Plackett–Luce）から推定します。Open Bandit Dataset は action_prob が付いていて貴重な例です。

IPS の落とし穴と改良  
- 支持不足（insufficient support）: 新ポリシーが出すアクションを旧ポリシーが一度も出していないと分母が0になり推定不可。対策は、探索用に確率的にランダム表示を混ぜるか全候補にゼロ以上の確率を与える設計。  
- 高分散: 旧ポリシーがほとんど出さなかったアクションがログで少数クリックされると、重要度比が大きくなり過大推定が生じる。対策としては「新旧ポリシーの差を小さくする」「重みを切り詰める（Clipped IPS, CIPS）」や「自己正規化（Self-Normalized IPS, SNIPS）」がある。

CIPS（クリッピング）  
重み $w_i=\frac{\pi_e}{\pi_0}$ を上限 $c$ でクリップして過大な重みを抑える。ただし $c$ の調整が必要。

SNIPS（自己正規化IPS）  
分母に重み合計を入れて正規化する方法で、パラメータ調整が不要：  
$$
\hat{V}_{SNIPS} = \frac{\sum_i r_i w_i}{\sum_i w_i},\quad w_i=\frac{\pi_e(a_i\mid x_i)}{\pi_0(a_i\mid x_i)}
$$
経験的には SNIPS が誤差が小さいことが報告されていますが、すべての観測で重みが必要になるため計算・格納コストは上がります。

他の手法  
- Direct Method（DM）: 欠測報酬を予測するモデルを作る。ログが少ない場合に有利だが、データ量が増えると IPS 系が強くなる傾向。  
- Doubly Robust: DM と IPS/SNIPS を組み合わせて頑健性を狙う。

学術的・実務的ポイント  
- A/Bテストは介入の最も確実な評価だが時間やリスクがかかる。反事実評価はその代替・予備評価として有用。  
- ただし反事実評価は「ログが十分に探索的である」こと、そして「推定に必要な確率が信頼できる」ことが前提。これが満たされない場合はバイアスや高分散に悩まされる。

## 実践ポイント
- 推薦確率（action_prob）をログに必ず保存する設計にする。後で IPS 系手法を使うなら必須。  
- 本番ポリシーにごく一部の探索ランダム化（あるいは確率的サンプリング）を混ぜ、支持不足を緩和する。PMと相談してKPIリスクを最小化する割合を決める。  
- 新しいモデルを評価する初期段階では SNIPS を優先検討する（パラメータ調整不要で安定しやすい）。ただしストレージ/計算コストに注意。  
- 重みのばらつきが大きい場合はクリッピングやモデルの近似化（旧ポリシーに近い探索）で分散を下げる。  
- 反事実評価は A/Bテストの代替ではなく補完。重要な変更はやはり小規模A/Bで検証する。  
- 日本市場向け実務例：楽天やYahoo!、LINE等で多数の商品・広告を扱う場合、クリック率の偏りや位置バイアスを補正するために印象ベースのprobログは特に役立つ。法規制やユーザー体験を損なわない程度のランダム化設計を心がける。

補足の読み物・データセット  
- Open Bandit Pipeline / Open Bandit Dataset（action_prob付き）  
- SIGIR/RecSys のチュートリアルや「IPS」「CIPS」「SNIPS」の原著論文

まとめ  
従来のオフライン評価は比較しやすく手軽だが、レコメンドの介入性を無視している面がある。反事実評価（特に SNIPS）を導入すれば、A/Bを回す前に効果の見積もりができ、意思決定の精度が上がる。ただし確率ログの整備や探索設計、分散対策など実務的配慮が不可欠です。
