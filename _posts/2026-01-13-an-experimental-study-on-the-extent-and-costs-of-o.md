---
layout: post
title: "An experimental study on the extent and costs of overreliance on AI - AIへの過剰依存の範囲とコストに関する実験的研究"
date: 2026-01-13T03:48:30.997Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://www.sciencedirect.com/science/article/pii/S0747563224002206"
source_title: "Trust and reliance on AI — An experimental study on the extent and costs of overreliance on AI - ScienceDirect"
source_id: 428179092
excerpt: "AIだからと過信して判断を誤り、個人と第三者に損害を与える過剰依存の実証研究"
---

# An experimental study on the extent and costs of overreliance on AI - AIへの過剰依存の範囲とコストに関する実験的研究
クリックせずにはいられない──「AIが言ったから」で損をする時代に、私たちが今すぐ知っておくべきこと

## 要約
AIからの助言が「AIである」と分かるだけで、人はその助言を過度に頼り、手元の情報や自分の判断と矛盾していても従ってしまう。結果として助言を受けた本人だけでなく第三者にも悪影響が及ぶ場合があるという実験的証拠が示された。

## この記事を読むべき理由
AI導入が進む日本の企業やサービスでは、意思決定の現場にAIが入り込む場面が増えています。どれだけ精度が高く見えても、人が「AIだから」と過信するとコストや社会的悪影響を招く可能性があるため、技術者・マネージャー・ユーザーの双方にとって必読の示唆があります。

## 詳細解説
本研究は、ドメインに依存しない（分野横断的な）インセンティブ付きの対話型行動実験を用いて、以下の点を明らかにしています。

- 「AIが生成した助言である」と認識しているだけで、人はその助言に従う傾向が強まる（＝ラベル効果）。つまり、助言の出典がAIであるという情報自体が判断に影響する。
- 被験者は、手元にある文脈情報や自分の評価と矛盾するAI助言でも従ってしまい、その結果、本人の効用（利益）が減少することが多い。
- 過剰依存は個人の損失に留まらず、他者との協力関係や第三者への影響を悪化させるケースが観察された。たとえば、誤ったAI助言を基に行動したことでチーム全体の効率が落ちる、といった外部不経済が生じる。
- 個人の「アドバイザーへの信頼（態度）」が高いほど、実際の「依存行動（振る舞い）」も強くなる相関が確認された。言い換えれば、AIへの好意的態度が過信につながりやすい。

研究は、アルゴリズム賛美（algorithm appreciation）と呼ばれる現象と、信頼の校正（trust calibration）の必要性を背景に議論しています。AIは意思決定の補助として有力ですが、誤りやバイアスを含むことがあり、単なる導入では問題が解決しないことを指摘しています。

## 実践ポイント
- AIの出所ラベルを付けるときは注意する：単に「これはAIの助言です」と示すだけで過信を招くため、助言の根拠・不確実性を同時に提示する設計を。
- 不確実性と説明（uncertainty + explanation）を表示する：確率・信頼区間やモデルが重視した根拠を可視化して、ユーザーが自分で評価できるようにする。
- 人間の最終判断を必須にするワークフロー：特に金融・医療・法務などリスクが高い分野では「人間の確認」を組み込む。
- AIリテラシー教育を導入：開発者だけでなく、現場の意思決定者や一般ユーザーにも、AIの限界やバイアスについての短時間トレーニングを実施する。
- 信頼の校正をモニタリング：ユーザーの「AIへの信頼度」と実際の依存行動を定期的に測り、過度な依存が見られたら介入する。
- 第三者影響の評価を行う：システム導入前に、ユーザー以外に及ぶ影響（チーム、顧客、社会）を想定したリスク評価を実施する。

日本市場への短い示唆：国内では顧客サービスや金融でのレコメンド、自治体の意思決定補助などAI導入が進んでいます。ユーザーが「AIだから正しい」と無条件で受け入れる文化にならないよう、企業はUI/UXと教育で信頼を設計する必要があります。
