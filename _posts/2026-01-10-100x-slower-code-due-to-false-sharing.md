---
layout: post
title: "100x Slower Code due to False Sharing - キャッシュライン競合で100倍遅くなる話"
date: 2026-01-10T07:24:45.725Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://www.youtube.com/watch?v=WIZf-Doc8Bk"
source_title: "100x Slower Code due to False Sharing - YouTube"
source_id: 466551349
excerpt: "スレッド競合のFalse Sharingで最大100倍遅延、即効パディング対策を紹介"
image: "https://i.ytimg.com/vi/WIZf-Doc8Bk/maxresdefault.jpg"
---

# 100x Slower Code due to False Sharing - キャッシュライン競合で100倍遅くなる話
ソケットやスレッド数が増えるほど「気づかぬうちに」大惨事になる、キャッシュラインの落とし穴を分かりやすく解説

## 要約
同じCPUキャッシュライン上の別スレッドが異なるデータを書き込むと、キャッシュコヒーレンシ処理で頻繁にデータが無効化されて性能が著しく低下する（いわゆる「False Sharing」）。適切な配置（パディング／アライン）やスレッド局所化で100倍レベルの改善が得られることがある。

## この記事を読むべき理由
マルチコア時代の今、ローカルで動くベンチやクラウドのスループットが思ったほど伸びない原因は意外と「メモリ配置」にあることが多い。日本のサービス運用やコスト最適化（クラウドの vCPU 数や課金）に直結する問題なので、初級エンジニアでも理解して対処できるように解説する。

## 詳細解説
- 問題の本質  
  CPU はデータをバイト単位で扱うが、キャッシュは「キャッシュライン」（多くは64バイト）単位で転送・保持する。スレッドAがあるアドレスを書き、スレッドBが同じキャッシュライン内の別アドレスを書こうとすると、各コアは相手のキャッシュを無効化／更新する必要があり、このやり取り（コヒーレンシプロトコル）で大量の遅延が発生する。書き込みが短時間に頻発すると「ping-pong」状態になり、性能が劇的に落ちる。

- なぜ100xにもなるか  
  単一スレッドでのキャッシュヒット中心の処理と、false sharing による継続的なキャッシュライン無効化ではCPUサイクルを使う割合が桁違い。特に短いループで頻繁に更新するカウンタやフラグに対して起きやすく、スループットが線形に増えないどころか大幅に悪化する。

- 実例（概念）  
  複数スレッドがインクリメントする単純な配列カウンタを例えば struct の配列で並べた場合、隣接する要素が同じキャッシュラインに収まると競合が発生する。

- 言語別ポイント  
  - C/C++: alignas(64) や明示的なパディングで解決。  
  - Java: HotSpot は @Contended（jdk のオプションで有効化）や手動パディングで対処。  
  - Go: runtime/internal/atomic が cachelinePad を提供するパターンがある。  
  - Rust: #[repr(align(64))] や余分なフィールドでパディング可能。

- 検出方法  
  - スケーリングテスト：スレッド数を増やしても性能が伸びない、あるいは悪化する。  
  - プロファイラ／ハードウェアカウンタ：cache-misses, LLC-load-misses が異常に多い。Linux の perf、Intel VTune、Java Flight Recorder などで確認。  
  - 最小再現コードで配列要素を分離して試すと簡単に確認できる。

## 実践ポイント
- すぐに試せる対策
  1. スレッドローカルなデータにする（可能ならグローバルカウンタを避ける）。  
  2. 要素ごとにキャッシュライン境界でアライン／パディングする。例（C++）:
```c
struct alignas(64) PaddedCounter {
    std::atomic<uint64_t> cnt;
    char pad[64 - sizeof(std::atomic<uint64_t>)];
};
```
  3. バッチ更新やフリービーフを使い、頻繁な共有書き込みを避ける（ローカルで集約して定期的に集約する）。  
  4. 言語固有の機能を使う（Java の場合は @Contended／手動パディング、Go は cachelinePad など）。  
  5. 本番と同等の負荷でスケールテストを行い、perf 等でキャッシュ関連カウンタを観測する。

- 測定のコツ
  - 小さな最小再現ケースを作って比較（パディングあり／なし）。  
  - perf stat -e cache-misses, LLC-load-misses などで差を確認。  
  - クラウド環境では vCPU 配置や NUMA による影響もあるので、コア配置の固定（CPU affinity）で検証する。

まとめ：見落とされがちなメモリ配置の最適化で、簡単に数倍〜100倍の差が出ることがある。まずはスケーリングテストとキャッシュ関連カウンタの確認を行い、必要ならパディング／局所化を施すこと。日本のサービス運用コスト低減にも直結するため、マルチコア性能に悩んだら真っ先に疑うべき問題である。
