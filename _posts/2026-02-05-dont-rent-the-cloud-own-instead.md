---
layout: post
title: "Don't rent the cloud, own instead - クラウドを借りるな、自分で所有せよ"
date: 2026-02-05T07:19:57.588Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://blog.comma.ai/datacenter/"
source_title: "Owning a $5M data center - comma.ai blog"
source_id: 46896146
excerpt: "累計$5Mでクラウド脱却、comma.aiの自前DC全貌（電力・冷却・GPU運用）"
image: "https://blog.comma.ai/img/datacenter/power.png"
---

# Don't rent the cloud, own instead - クラウドを借りるな、自分で所有せよ

魅力的なタイトル案：月々のクラウド請求書に泣かない方法 — comma.aiが$5Mで作った“自前”データセンターの全貌

## 要約
comma.aiはクラウド依存を脱して約500万ドルで自社データセンターを構築し、トレーニングや推論、ストレージを一括で運用してコストとエンジニアリングの質を大きく改善している。

## この記事を読むべき理由
クラウドコストに悩む日本のスタートアップ／研究チームにとって、自前インフラが現実的であることと、実際に何が必要か（電力・冷却・ネットワーク・ソフト）を具体的に知れるから。

## 詳細解説
- なぜクラウドを避けるか  
  - オンボーディングは簡単、オフボーディングは難しい（ベンダーロックイン）。予算で問題を先送りしがちで、最適化インセンティブが働きにくい。自前ならワットやFLOPsを直接最適化する文化になる。

- 全体像（comma.aiの実装）  
  - コスト感：累計約$5M。クラウドで同等運用なら$25M超の見積もり。  
  - 電力：ピークで約450kW。電気代が大きな運用費（地域で大きく変動）。  
  - 冷却：CRACではなく「外気冷却＋再循環」で低消費電力を実現。温湿度はセンサ＋PIDで制御。  
  - サーバ／GPU：600 GPUsを75台の自社設計TinyBox Pro（各2CPU＋8GPU）で運用。故障対応は自力で迅速化。  
  - ストレージ：DellベースのSSD群で合計約4PB。スループット重視で主要配列は冗長無し（用途に応じ冗長配列も有）。ランダムリードで各チャンク20Gbpsを目指す設計。  
  - ネットワーク：3台の100Gbpsコアスイッチ＋InfiniBandでトレーニング用All-Reduceを構成。  
  - ソフトウェア：PXE＋Saltでプロビジョニング。mkv（minikeyvalue）で大容量分散ストレージ、Slurmでジョブ管理、PyTorch＋FSDPで分散学習。独自の軽量タスクスケジューラ「miniray」で雑多な分散処理を回す。  
  - 開発ワークフロー：NFSキャッシュされたモノレポをワークステーション毎に同期し、ジョブ開始時にローカルの正確なコードとパッケージを2秒程度で配布。実験管理は自前のトラッキングサービス（UUIDで重み管理）。

- 技術的注目点  
  - 外気冷却＋PID制御で冷却電力を大幅削減（気候条件に依存）。  
  - 非冗長で高速なストレージ配列を許容する運用方針（データの性質で判断）。  
  - Slurm＋InfiniBand＋FSDPの組合せで大規模分散学習を低コストに実現。  
  - minirayのような軽量分散タスク基盤は、空きGPUを効率利用するうえで有効。

## 実践ポイント
- 小さく始める：まずは1〜2ラック、自社運用のTCO（電気、冷却、スペース、保守）を算出する。  
- 電力と気候を最優先で評価：電気料金・契約・外気冷却の可否は導入可否を左右する。  
- オープンツールを活用：Slurm、PyTorch（FSDP）、NFS、Redisなどで試作を素早く回す。  
- ストレージ設計は用途で最適化：高速ランダムリードが必要ならSSDと非冗長高速配列、重要データは冗長化。  
- 運用文化を作る：ハードウェア故障対応、センサ監視、電力/温湿度ログの運用体制を構築することが成功の鍵。  
- コード同期と環境再現：monorepo＋パッケージ同期で分散実行時の「動作差分」を無くす。  
- 法規・契約確認：日本では設置許可、電力契約、消防・建築要件など確認を忘れずに。

興味があるなら、まずは小規模なPoC（1ラック）から始め、Slurm＋PyTorchで分散学習を一度自前で回してみると、コスト感と運用の可否が掴めます。
