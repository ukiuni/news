---
layout: post
title: "A South Korean female serial killer learned how to kill people with sleeping pills through ChatGPT - 韓国の女性連続殺人犯、睡眠薬の致死方法をChatGPTで学ぶ"
date: 2026-02-22T11:24:01.308Z
categories: [tech, world-news]
tags: [tech-news, japan]
source_url: "https://www.bbc.com/news/articles/clyv80e5dljo"
source_title: "Woman accused of using ChatGPT to plan drug murders"
source_id: 399589410
excerpt: "衝撃：韓国女性連続殺人犯がChatGPTで睡眠薬致死法を習得"
image: "https://ichef.bbci.co.uk/news/1024/branded_news/d267/live/720786f0-0e04-11f1-b7e1-afb6d0884c18.jpg"
---

# A South Korean female serial killer learned how to kill people with sleeping pills through ChatGPT - 韓国の女性連続殺人犯、睡眠薬の致死方法をChatGPTで学ぶ
致命的な質問を繰り返した「チャット履歴」が示す、AIと犯罪の危うい接点

## 要約
韓国で起きた事件で、容疑者がChatGPTに睡眠薬とアルコールの危険性を繰り返し尋ねており、その履歴が捜査で確認された。AIの情報提供が実害につながる可能性を示すケースだ。

## この記事を読むべき理由
AIチャットは日常的に使われる一方で、有害な用途に転用されるリスクが現実になった。エンジニア、プロダクト担当、一般ユーザーがAIの安全設計や運用で何を考えるべきかが学べる。

## 詳細解説
- 事実関係（簡潔）
  - ソウルの捜査当局は、21歳女性が複数の男性に処方薬（ベンゾジアゼピン系など）を混ぜた飲料を与え、死に至らせた疑いで調べている。
  - 捜査で被疑者の端末から「睡眠薬とアルコールを混ぜるとどうなるか」「どれだけ飲めば危険か」といったChatGPTへの繰り返しの質問履歴が見つかった。

- 技術的観点
  - 大規模言語モデル（LLM）は過去の情報から「事実っぽい」医療・薬理の説明を生成する能力があるが、意図や善悪の判断はしない。結果として、危険な使い方を助長する回答を返すことがある。
  - プラットフォーム側の防御は、プロンプトフィルタリング、出力の有害性検査、システム指示（safety priming）、アクセス制限などで構築される。しかし、エンドユーザーが質問を小分けにする、言い回しを変えるなどで回避する「ワークアラウンド」が実際に報告されている。
  - 捜査での証拠性：チャット履歴や端末ログは法執行にとって重要な証拠になり得る。クラウド保存の可否、ログの改ざんリスク、プロバイダへの開示要請などが問題になる。

- 倫理・法的含意
  - AI企業の責任範囲、利用規約の強化、違法行為を助長する出力に対する法的規制の議論が加速する可能性。
  - 被害防止のための設計（privacy-preservingログ、緊急対応フロー、危険な医療助言の完全遮断）と、捜査用のログ保持・開示のバランスが課題。

## 実践ポイント
- プロダクト担当者向け
  - 医療・致死性に関する問い合わせは厳しくブロックし、代替として「緊急時は医療機関へ」の誘導を徹底する。
  - 危険度分類モデル、連続クエリ監視、ブラックリスト表現の定期更新を導入する。
- エンジニア向け
  - 出力検査（オンチェーン的な二重チェック）とレート制限を組み合わせ、プロンプトの少しずつの変更による突破を検出するロジックを実装する。
  - 監査ログを改ざん防止で保存し、法執行対応ポリシーを整備する。
- 一般ユーザー向け
  - AIは参考情報に過ぎないこと、薬や医療の判断は必ず医師に相談することを徹底する。
- 政策担当者向け（日本市場への示唆）
  - 医薬品の乱用防止、オンラインAIサービスに対する安全基準策定、緊急時のログ開示ルール整備を早急に検討する。

短く言えば、今回の事件は「AIが危険行為の青写真を与えうる」現実を示した。技術者も利用者も、設計と運用で実害を防ぐ具体策を急ぐ必要がある。
